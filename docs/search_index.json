[["index.html", "Tree phenology analysis with R Chapter 1 Introduction to the Module 1.1 Learning goals", " Tree phenology analysis with R Jacqueline Wingen 04 März 2025 Chapter 1 Introduction to the Module Hi, my name is Jacqueline. I’m a master’s student in Crop Sciences at the University of Bonn. This is my learning logbook for the module “Tree phenology analysis with R”. This module provides an overview of methods to study the impact of climate and climate change on tree phenology. It is designed for those who may not yet be familiar with phenology or how to analyze climate change effects, but it also aims to offer new insights for those with existing knowledge in these areas. Initially developed for M.Sc. students in Crop Science and Agricultural Science and Resource Management in the Tropics and Subtropics (ARTS) at the University of Bonn, the material is accessible to anyone interested. The content begins with an introduction to phenology (with a special emphasis on dormancy) as well as an overview of climate change. It then focuses heavily on the practical application of the chillR package for R. This tool has been continuously developed since 2013 by Prof. Dr. Eike Lüdeling, head of the HortiBonn research group at the Institute of Crop Science and Resource Conservation (INRES) at the University of Bonn, to support this type of analysis. 1.1 Learning goals This course will offer the following skills and experiences: Knowledge about phenology Knowledge about tree dormancy Understanding of climate change impact projection methods Appreciation for the importance of risks and uncertainty in climate change projection Understanding of how to use some staple tools of R code development Ability to use chillR functions for climate change impact projection Ability to use chillR functions for tree phenology analysis Understanding and ability to use the PhenoFlex dormancy analysis framework "],["tools.html", "Chapter 2 Tools 2.1 R and RStudio 2.2 Git and Github 2.3 Rmarkdown", " Chapter 2 Tools This course is designed to provide knowledge about tree phenology, climate change, and related topics, along with hands-on exercises to demonstrate the functionalities of the chillR package. It is recommended to document everything learned in a learning logbook. To engage in these practical components effectively, various tools are required. Since chillR is an R package, using R, preferably through the RStudio interface, will be necessary. Although it is possible to run RStudio on a local computer and save files directly on the hard drive, this approach differs from the methods commonly used by professional programmers. To align with standard programming practices, familiarity with certain code development tools is essential. This course will therefore cover the basics of using Git and GitHub, which are valuable tools for organizing, securing, and sharing code. Additionally, proper documentation techniques in R will be introduced, focusing on creating well-structured, professional reports using RMarkdown. While these tools may seem complex at first, their usefulness is likely to become clearer as they are used throughout the module. Dr. Cory Whitney, a researcher at HortiBonn, has volunteered to create tutorial videos to provide an introduction to these tools. 2.1 R and RStudio The first video Using R and RStudio demonstrates how to install and run R and RStudio: 2.2 Git and Github The next video Using Git and Github explores the programming version control environment Git and the interface GitHub, which is used to access these features: 2.3 Rmarkdown In the last video Using R Mardown, R Markdown will be examined, a powerful tool that enables the creation of sophisticated reports, websites, and more from R code: "],["tree-dormancy.html", "Chapter 3 Tree dormancy 3.1 Learning goals 3.2 Introduction to dormancy 3.3 Dormancy physiology 3.4 Experimental and statistical determination of chilling and forcing periods 3.5 Phenology record and BBCH scale 3.6 Excercises on tree dormancy", " Chapter 3 Tree dormancy This chapter is presented by Dr. Erica Fadón, a researcher at HortiBonn from 2018 to 2021, who explores dormancy in temperate fruit trees. This topic remains complex and has many unanswered questions. A central question for researchers is, “How do trees know when to flower?” Although it seems clear that trees bloom in spring, the reality is more complicated. This chapter provides a better understanding of dormancy and demonstrates how to use the chillR tool to predict flowering times, even in the face of challenges posed by global warming. 3.1 Learning goals Learn about dormancy in temperate fruit trees Be able to identify the phenological stages of a fruit tree and understand phenology data sets Describe and compare the two methodologies (empirical and statistical) to identify the chilling and forcing periods of a certain cultivar 3.2 Introduction to dormancy Tree dormancy is a state of reduced activity that occurs when environmental conditions are unfavorable, especially during winter. This state acts as a survival strategy, helping trees withstand extreme temperatures, water shortages, and other stress factors. During dormancy, trees slow down or stop their growth to conserve energy and avoid damage. Dormancy is a continuous process divided into three phases: Dormancy Establishment Endo-Dormancy Eco-Dormancy Dormancy establishment is the process where temperate trees transition from active growth in summer to a dormant state in autumn. This shift is mainly triggered by shorter daylight hours and decreasing temperatures, causing buds to form, growth to stop, and leaves to fall. The importance of these factors varies by species, with some trees responding more to day length and others to temperature. Endo-dormancy is a phase of dormancy controlled by the plant’s internal factors, where growth is suppressed even under favorable conditions. It requires a period of cold exposure (chilling) to release the buds from this state, preventing premature growth during temporary warm spells in winter. Low temperatures are the main trigger for breaking endo-dormancy, while the role of light (photoperiod) is still uncertain. Eco-dormancy is the phase after endo-dormancy, where buds have regained their ability to grow but remain inactive due to unfavorable environmental conditions, mainly low temperatures. During this phase, growth is on hold until warmer temperatures trigger it. Heat accumulation is needed to resume growth. Eco-dormancy ends when enough heat has been accumulated, leading to visible growth changes, typically in late winter or early spring. The below video Introduction to dormancy by Dr. Erica Fadón gives the basic knowledge of this dormancy phases and processes that regulate dormancy. 3.3 Dormancy physiology Dormancy as a whole is the result of complex interactions between numerous physiological processes that occur in different parts of the tree, such as buds, twigs, meristems, and vascular tissues. We divide these processes into four main themes: Transport: occurs at both the whole-plant and cellular levels Phytohormone Dynamics: behavior and levels of plant hormones during dormancy Genetic and Epigenetic Regulation: how genetic factors and their modifications influence dormancy Dynamics of Nonstructural Carbohydrates: changes in carbohydrate levels that affect dormancy The following figure from the study “A conceptual framework for Winter Dormancy in Deciduous Trees” by Fadón et al. (2015) presents a conceptual framework of winter dormancy in deciduous trees and summarizes the three dormancy phases along with their respective physiological processes. Conceptual framework of winter dormancy in deciduous trees. The dormancy framework (gray background) indicates three main phases: (a) dormancy establishment (light gray), (b) endo-dormancy (dark gray), and (c) eco-dormancy (medium gray). For each phase (a–c), the dormancy-related physiological processes are represented by colored shapes and numbers (0 to 4). All the processes depicted are explained in detail in the video below, Dormancy Physiology by Dr. Erica Fadón. 3.4 Experimental and statistical determination of chilling and forcing periods Dormancy consists of two phases where temperatures have opposite effects on flowering. During endodormancy, higher chill accumulation leads to earlier flowering, whereas similar cool temperatures during ecodormancy can delay flowering. The challenge lies in differentiating between these two phases, as the tree buds appear to be in the same developmental stage throughout. To address this, there are two methods available: Experimental method: collecting buds periodically during winter, exposing them to favorable growth conditions, and evaluating bud break to determine when dormancy is overcome Statistical method: uses long-term phenological data and temperature records to estimate the dates of chilling fulfillment and heat accumulation through partial least squares (PLS) regression analysis The video Dormancy determination covers the experimental and statistical methods to determine the chilling and forcing periods for temperate fruit trees to overcome dormancy and initiate growth. It explains the concept of dormancy, its phases (endodormancy and ecodormancy), and the temperature requirements for breaking dormancy. 3.5 Phenology record and BBCH scale Phenology is the study of periodic events in biological life cycles and how these are influenced by seasonal and interannual variations in climate. This module will involve working with phenology data sets, primarily focusing on a specific stage, usually budbreak, even though trees pass through various developmental stages during the year. These stages are typically identified by numerical codes. To describe these growth stages systematically, the BBCH scale is employed. This internationally standardized system outlines the growth and developmental phases of plants. The BBCH scale consists of ten main stages, known as principal growth stages, which are numbered from 0 to 9. Each of these main stages is further divided into substages, enabling a more detailed description of a plant’s development. Principal growth stages: Stage Description 0 Germination / sprouting / bud development 1 Leaf development (main shoot) 2 Formation of side shoots / tillering 3 Stem elongation or rosette growth / shoot development (main shoot) 4 Development of harvestable vegetative plant parts or vegetatively propagated organs / booting (main shoot) 5 Inflorescence emergence 6 Flowering (main shoot) 7 Development of fruit 8 Ripening or maturity of fruit and seed 9 Senescence, beginning of dormancy For a comprehensive overview of phenology and the BBCH scale, the video Phenology by Dr. Erica Fadón is recommended. In this video, Dr. Fadón explains the concept of phenology and how the BBCH scale uses numerical codes to represent the different developmental stages of trees, from budding and flowering to fruit ripening and leaf fall. 3.6 Excercises on tree dormancy Put yourself in the place of a breeder who wants to calculate the temperature requirements of a newly released cultivar. Which method will you use to calculate the chilling and forcing periods? Please justify your answer. As a breeder aiming to calculate the temperature requirements for the chilling and forcing periods of a newly released cultivar, I would use the experimental method to determine the chilling and forcing periods. Here’s my justification: Direct measurement of bud response: The experimental method allows me to directly observe when buds break under controlled temperature conditions. By regularly collecting buds during winter and placing them in ideal growth conditions, I can determine exactly when dormancy ends. This practical approach gives me quick and useful information about the specific cultivar Specific to the cultivar: Each cultivar has its own unique chilling and forcing needs. The experimental method looks at the specific traits of the new cultivar, making sure the results are relevant and applicable to that variety Immediate results for breeding decisions: The experimental method provides quick evaluations of bud break, allowing me to make faster decisions about breeding and managing the new cultivar Which are the advantages (2) of the BBCH scale compared with earlier scales? Standardization: The BBCH scale provides a standardized framework for describing plant growth stages, enabling consistent communication and comparisons across studies Detailed Staging: It offers a more granular categorization of developmental stages using a two-digit system, allowing for a better understanding of plant development and environmental impacts. Classify the following phenological stages of sweet cherry according to the BBCH scale: left image: BBCH stage 55 (single flower buds visible (still closed), green scales slightly open) middle image: BBCH stage 65 (full flowering: at least 50% of flowers open, first petals falling) right image: BBCH stage 89 (fruit ripe for harvesting) "],["climate-change-and-impact-projection.html", "Chapter 4 Climate change and impact projection 4.1 The drivers of climate change 4.2 What’s already known 4.3 Future scenarios 4.4 Impact projections approaches 4.5 Exercises on climate change", " Chapter 4 Climate change and impact projection Before using chillR, there’s a brief overview of climate change, because the upcoming work will mainly focus on predicting how global warming might affect phenology-related metrics. Climate change refers to long-term changes in temperatures and weather patterns. While these changes can occur naturally — such as fluctuations in solar activity — since the 19th century, climate change has primarily been driven by human activities, particularly the burning of fossil fuels like coal, oil, and natural gas. 4.1 The drivers of climate change To understand what’s happening to our planet, it’s important to know the main causes of climate change. This helps us spot false claims that things like the sun, cities, or natural changes in the climate are the main reasons for global warming. The truth is simple: human-made greenhouse gas emissions are heating up our planet, and the only way to stop this is to greatly reduce these emissions. The video below, titled Climate Change 1 - Drivers of Climate Change, is the first in a series of four videos on the topic of climate change presented by Eike Lüdeling. It provides a comprehensive overview of the primary drivers of global climate change, such as greenhouse gases, aerosols, solar radiation, ozone, and others. 4.2 What’s already known The next video, Climate Change 2 - Recent Warming, explores climatic changes that have already occurred or for which there is substantial evidence. It demonstrates that the planet has experienced significant warming for several decades, almost globally. 4.3 Future scenarios When it comes to climate change, the most severe impacts are still ahead. This is largely due to the significantly higher rate of greenhouse gas emissions observed over the past few decades, with no signs of a slowdown in the near future. As a result, the human-induced ‘forcing’ effect on our climate has reached unprecedented levels, making it likely that future changes will occur even more rapidly than those we have already witnessed. The next video Climate Change 3 - Future scenarios introduces the methods that climate scientists employ to forecast future conditions and presents climate scenarios developed by these scientists, which researchers in other fields can use to project the impacts of climate change on ecological and agricultural systems. 4.4 Impact projections approaches Having robust climate scenarios is essential, but they only take us partway toward reliable assessments of climate change impacts. A potentially greater challenge lies in translating these climate scenarios into biological consequences. To achieve this, we need impact models or other methods to derive the impacts of climate change. The last video Climate change 4 - impact projection approaches introduces various methods for projecting climate impacts. 4.5 Exercises on climate change List the main drivers of climate change at the decade to century scale, and briefly explain the mechanism through which the currently most important driver affects our climate. The main drivers of climate change on a decade-to-century scale include: Greenhouse Gases (GHGs): GHGs like carbon dioxide (CO₂), methane (CH₄), and nitrous oxide (N₂O) trap heat in the atmosphere, leading to the greenhouse effect, which raises Earth’s temperature. The increase in these gases is primarily due to human activities, such as burning fossil fuels, industrial processes, and deforestation Aerosols: Particles in the atmosphere that can cool the climate by reflecting sunlight. They come from both natural sources (e.g. sea salt, dust, volcanic eruptions, fires) and human activities (e.g.power plants, cars, fires and cook stove). They are major climate driver in industrial centers (e.g. China) Sun: Solar radiation heats the Earth, with minor fluctuations occurring over time due to cycles in solar activity, such as sunspots. Although these variations contribute only a small portion to the current climate changes, they play a significant role in driving climate change over geological timescales Ozone: Ozone in the stratosphere protects Earth from UV-B radiation, while tropospheric ozone acts as a greenhouse gas and contributes to warming Surface albedo: The reflectivity of the Earth’s surface affects how much solar energy is absorbed. Light surfaces (like ice) reflect more energy, while dark surfaces (like forests or oceans) absorb more, influencing the planet’s heat balance. Changes in surface reflectivity, such as melting ice and snow, decrease the albedo effect, leading to more heat absorption and further warming The currently most important driver of climate change is greenhouse gases, particularly CO₂. The mechanism through which CO₂ affects the climate involves the greenhouse effect: CO₂ molecules in the atmosphere absorb long-wave radiation emitted from the Earth’s surface and re-radiate it in all directions, including back toward the surface. This process traps heat and increases global temperatures, driving many of the changes we observe in climate patterns. Explain briefly what is special about temperature dynamics of recent decades, and why we have good reasons to be concerned. In recent decades, global temperatures have been rising at a faster rate than at any other time in human history. This trend is evident from the fact that the hottest years on record have all occurred within the last few decades. One striking example is the extreme heat in Siberia in the spring of 2020, where temperatures were up to 8°C above the recent average. This trend is particularly concerning because it is mainly driven by human activities, especially the emission of greenhouse gases. Unlike previous climate changes, which took place slowly over long periods, today’s fast rise in temperatures increases the risk of triggering dangerous effects, like melting permafrost and losing ice cover, which could make global warming even worse. Even a small increase of 1.5°C could seriously upset the balance of our climate, showing how important it is to take action against these human-caused changes. What does the abbreviation ‘RCP’ stand for, how are RCPs defined, and what is their role in projecting future climates? RCP stands for Representative Concentration Pathways, which are essential scenarios used in climate modeling to project potential future greenhouse gas emissions and their impacts on the climate. RCPs are defined by the level of radiative forcing — measured in watts per square meter (W/m²) — that is expected by the end of the 21st century. Each pathway corresponds to a specific amount of greenhouse gas concentrations, which can significantly influence global temperatures. The role of RCPs is to serve as inputs for climate models, helping to produce future climate scenarios, which are essential for understanding the potential impacts of climate change and planning appropriate mitigation and adaptation strategies. Briefly describe the 4 climate impact projection methods described in the fourth video. The four climate impact projection methods described in the fourth video are: Statistical models: These models establish relationships between climate parameters and impact measures, such as crop yield. They use historical data to explain past trends and project future climate impacts. Their primary limitation is that the statistical relationships may not remain valid under future climate conditions, and they may overlook important factors Species Distribution Modeling: Also known as ecological niche modeling, this method predicts the future distribution of species by relating current presence or absence data to climatic parameters. However, these models may assume species are in equilibrium with the climate, which is often not the case Process based models: These models aim to represent all major system processes using equations, capturing the scientific knowledge of processes like crop growth, phenology or hydrology. However, they are limited by the lack of complete understanding of complex systems, and often require extensive parameterization or assumptions Climate Analogue models: This method identifies current locations with climates similar to those expected in the future at another site, offering real-world examples that can guide adaptation strategies. However, they may be limited by differences in non-climatic factors and lack of suitable data, making it difficult to draw clear conditions "],["winter-chill-projections.html", "Chapter 5 Winter chill projections 5.1 Learning goals 5.2 Winter chill in Oman 5.3 Chill model sensitivity 5.4 Winter chill in California 5.5 Winter chill ratios 5.6 A global projection of future winter chill 5.7 Winter chill in Germany 5.8 Winter chill in Tunisia 5.9 Winter chill in Chile 5.10 Chill projection for Patagonia 5.11 Chill model comparison 5.12 Chill projection for all of Tunisia 5.13 Revisiting chill accumulation in Oman 5.14 Exercises on past chill projections", " Chapter 5 Winter chill projections This section provides an overview of how winter chill can be modeled. It summarizes past studies on this topic, aiming to clarify the methodological aspects that lead to the analyses conducted. By the end of this lesson, most of the analyses presented in the discussed papers should be understandable. 5.1 Learning goals Be aware of past studies that have projected climate change impacts on winter chill Get a rough idea of how such studies are done Get curious about how to do such studies 5.2 Winter chill in Oman During his doctoral studies at the University of Kassel, Prof. Dr. Eike Lüdeling became interested in winter chill while participating in research on mountain oases in Oman. Initially focused on calculating nutrient budgets for the oases, particularly in the “Hanging Gardens” of Ash Sharayjah, the study shifted when many fruit trees failed to bear fruit. This led to the hypothesis that insufficient winter chill might be the issue, especially since the oases hosted temperate species such as pomegranates (Punica granatum), walnuts (Juglans regia), and apricots (Prunus armeniaca). To investigate this, temperature loggers were placed in three oases at different levels of elevation, allowing for the study of chill accumulation along an elevation gradient. A map of the study area illustrates the locations of the oases: Map of oasis systems in Al Jabal Al Akhdar, Oman A nearby long-term weather station provided valuable data, although its location - 1000 meters above the lowest oasis - limited its representativeness. Since records were available from the oases, transfer functions were defined to derive oasis temperatures from the long-term data. These transfer functions were set up using PLS regression, which, in hindsight, wasn’t a very good idea, to directly calculate hourly temperatures in the oases from the daily records of the official station at Saiq. Regression between temperature at Saiq and temperature in three oases, Al Jabal Al Akhdar, Oman This approach facilitated the calculation of hourly temperatures, which were essential for assessing winter chill dynamics over several years. Chill dynamics between 1983 and 2007, Al Jabal Al Akhdar, Oman The findings were submitted to the journal Climatic Change (Luedeling et al., 2009b), where reviewers suggested incorporating future climate scenarios. To address this, the LARS-WG weather generator was employed to simulate plausible weather conditions for the oases under scenarios of 1°C and 2°C warming. Chill prospects for 1°C and 2°C warming scenarios in Al Jabal Al Akhdar, Oman The results illustrated the potential impacts of climate change on winter chill, marking the beginning of a career focused on chill modeling. 5.3 Chill model sensitivity After completing a PhD at the University of Kassel, Prof. Dr. Eike Lüdeling became a Postdoctoral Scholar at the University of California at Davis, where his research focused on climate change impacts on winter chill in California’s Central Valley, a key region for temperate fruit tree production. Upon arriving in California, it became evident that the choice of chill model significantly impacts winter chill quantification. Initially, the simplest model was chosen due to a lack of programming skills, but further investigation highlighted the importance of model selection. Extensive library research revealed the need for a thorough examination of various chill models. Knowledge gained in Oman was utilized to create temperature scenarios for multiple locations, allowing for the analysis of how chill accumulation would likely change in the future. The analysis focused on changes predicted by various models for the same locations and future scenarios. Here are the locations examined: Weather station locations in California The results revealed considerable variation in chill projections for these locations. The analysis illustrated significant differences in estimates of chill losses by 2050, indicating that not all models could accurately represent winter chill dynamics. Ultimately, the Dynamic Model emerged as the most reliable option, prompting its primary use in subsequent research. Sensitivity of chill projections to model choice (CH - Chilling Hours; Utah - Utah Model; Utah+ - Positive Utah Model; Port. - Dynamic Model) However, challenges arose with the complexity of the Dynamic Model, which required outdated Excel software for calculations. Additionally, the data processing steps necessary to generate credible temperature scenarios proved cumbersome and error-prone, highlighting the need to develop programming skills for more efficient analysis. 5.4 Winter chill in California The primary goal during the time in California was to create a winter chill projection for the Central Valley, an important region for fruit and nut production. Utilizing California’s extensive network of weather stations, the plan involved using data from over 100 stations and generating multiple climate scenarios. To manage this complex task efficiently, a decision was made to automate most processes, leading to an exploration of programming. The automation was implemented using JSL, a programming language associated with the statistics software JMP, which facilitated the handling of the data. Despite some challenges, the automation was largely successful, though running the weather generator manually for each station remained tedious. Ultimately, projections were generated for all stations, illustrating chill accumulation over 100 plausible winter seasons for each climate scenario. To present the results effectively, a metric called ‘Safe Winter Chill’ was developed, defined as the 10th percentile of the chill distribution, indicating the minimum chill amount that would be exceeded in 90% of the years. Here’s an illustration of the Safe Winter Chill metric: Illustration of the Safe Winter Chill concept A method for spatially interpolating the station results was also established, leading to the creation of maps that depicted winter chill prospects for the Central Valley. Here’s one of the maps that resulted from this: Winter chill prospects for California’s Central Valley This analysis was published in the journal PLOS ONE (Luedeling et al., 2009d). 5.5 Winter chill ratios Following the automation of processing steps in JSL, attention turned to creating a global winter chill projection. The Global Summary of the Day database was identified as a valuable data source, featuring records from thousands of weather stations. The project proved challenging due to limited programming skills. Data processing was carried out on six computers operating around the clock for several weeks, likely a result of initial setup difficulties rather than the complexity of the analyses. In the end, data for about 5,000 weather stations were processed, generating multiple chill metrics. This extensive dataset allowed for a comparison of chill models by calculating the ratios between various chill metrics at each station. If these ratios had been consistent worldwide (e.g., one Chill Portion always equating to ten Chilling Hours), any chill model could have been reliably used. However, significant variations in chill metric ratios were observed globally. Chill metric ratios around the world This study was published in the International Journal of Biometeorology (Luedeling &amp; Brown, 2011a). 5.6 A global projection of future winter chill Using the same analytical methods, a global projection of the potential impacts of climate change on winter chill was also generated: Projected decline in available winter chill around the world The regions marked in red and orange on the lower two maps may experience significant impacts on fruit and nut production due to decreasing winter chill. With substantial chill losses, it is unlikely that growers will be able to sustain their current tree cultivars. Notably, the Mediterranean region is expected to be particularly affected. Winter chill projection for the Mediterranean region This prompted collaboration with partners in the Mediterranean region and other countries with similar climates, such as South Africa and Chile. 5.7 Winter chill in Germany Germany is not highlighted as particularly vulnerable to chill losses, and an analysis of historical chilling trends from 1950 supports this observation: Winter chill in Germany in 2010, and changes between 1950 and 2010 5.8 Winter chill in Tunisia Prospects for orchards in Tunisia are particularly challenging due to the region being close to the warmest limits for many fruit and nut tree species. An assessment published in 2018 examined past and future trends in winter chill for an orchard in Central Tunisia, following a seven-year gap from earlier studies. This delay stemmed from other professional commitments and the difficulty of obtaining suitable future climate data for chill modeling. While climate change data is widely available, much of it is presented as spatial grids, making it cumbersome to work with. Each climate scenario requires numerous grids for temperature and rainfall, leading to substantial data storage needs, sometimes exceeding 700 GB. Soon after establishing a processing structure for these datasets, the IPCC introduced the Representative Concentration Pathways (RCPs), rendering earlier scenarios outdated and complicating the analysis further, especially given the limited data transfer capabilities while based in Kenya. Collaboration with colleagues in Tunisia, particularly Haifa Benmoussa, revealed that tree crops like almonds and pistachios are highly vulnerable to climate change impacts. Fortunately, a new climate database specifically for Africa, called AFRICLIM, was developed, facilitating the acquisition and processing of relevant climate scenarios. This allowed for the incorporation of new functions in chillR to sample from AFRICLIM grids and produce the necessary climate projections. Winter chill analysis for an orchard near Sfax in Central Tunisia (blue bars indicate approximate chilling requirements of pistachios and two types of almonds) The figure, which is to be created by the end of the semester, illustrates the historical development of chill accumulation at a specific location, with observed values represented by red dots and typical chill distributions shown as boxplots. These data were generated using a weather generator that is calibrated with historical weather data and produces artificial annual weather records. The generator was also used to create future scenarios based on the AFRICLIM database. The analysis indicates that in none of the future scenarios does the cultivation of pistachios or high-chill almonds remain viable. This conclusion is supported by observations in Tunisia, where, after the warm winter of 2015/16, many pistachio trees barely developed any flowers, leading to crop failures. Pistachio tree near Sfax, Central Tunisia, in April of 2016 5.9 Winter chill in Chile AFRICLIM addressed the challenge of obtaining future climate data for Africa but did not fully meet the needs for integrating climate change projections into chillR. It was limited to African data, and users seeking information for single locations had to download large datasets, which was inefficient. A more effective solution was needed to access data quickly for individual weather stations globally. An early resource was ClimateWizard, developed by Evan Girvetz, which initially provided gridded data but later included a script for extracting information for specific locations. This functionality was eventually made available through an API at CIAT, allowing access to outputs from 15 climate models for the latest RCP scenarios. This advancement enabled Eduardo Fernández to analyze past and future chill development across nine locations in Chile, expanding the geographic scope of the research. Map of fruit growing locations in Chile The following diagram illustrates the assessment of past and future winter chill across nine locations in Chile: Assessment of past and future winter chill for 9 locations across Chile Eduardo preferred a different plot design and utilized the ggplot2 package, a robust plotting tool for R, to redesign it. The complexity of having data from multiple sites made interpretation challenging, prompting Eduardo to creatively summarize key information for each scenario. He presented this information as a heat map, simplifying the visualization. Heatmap showing Safe Winter Chill (10% quantile of chill distribution) for nine locations in Chile 5.10 Chill projection for Patagonia Certain regions may become more suitable for agriculture as the climate changes. An analysis was conducted to assess the climatic suitability for fruit and nut trees in Patagonia, southern Argentina, which is located at the southern frontier of agriculture: Map of parts of Patagonia in Argentina, showing locations that were analyzed in this study Weather station records for all locations on the map were obtained, enabling the calibration of a weather generator and the download of climate projections from the ClimateWizard database. This facilitated the creation of past and future temperature scenarios for all stations, as well as the computation of winter chill and other agroclimatic metrics. However, the results of the winter chill calculations were not particularly noteworthy, as minimal changes were projected. Heatmap showing Safe Winter Chill (10% quantile of chill distribution) for eleven locations in Patagonia Climate change could potentially enhance land suitability for fruit trees by providing increased summer heat: Past and projected future heat availability for four exemplary locations in Patagonia A further beneficial development is a likely reduction in the number of frost hours: Past and projected frost risk for four exemplary locations in Patagonia While the changes observed may appear minor, they are likely to shift many locations from a climate that is too cool for agriculture, particularly for fruit trees, to a more optimal situation. This presents a rare instance of potentially positive news related to climate change, though it is important to acknowledge that these changes could have negative consequences for natural ecosystems and other agricultural systems. 5.11 Chill model comparison Eduardo Fernandez recently utilized the climate change analysis framework to enhance previous chill model comparisons, significantly building on earlier work. He compiled a collection of 13 methods for quantifying chill accumulation from existing literature and applied these models to datasets from several locations in Germany, Tunisia, and Chile, which are part of the PASIT project. A map illustrates the locations included in this analysis. Locations used for comparing predictions by a total of 13 chill models across different climates The expectation was that the models would show significant differences in the extent of changes they predicted, and this anticipation was indeed fulfilled: Chill change projections by a total of 13 chill models across different climate scenarios The figure illustrates the changes predicted by 13 different models across various sites and climate scenarios, categorized into three groups: warm, moderate, and cool. Eduardo’s analysis reveals significant discrepancies among the models, highlighting the risks of selecting the most convenient model for predictions. The variation in predictions is evident in the color distribution across the rows of the panels, with a uniform color indicating consistency among models—something that is not observed here. For locations in Tunisia and Chile, the predictions mainly concern the extent of chill losses, ranging from mild to alarming. In Germany, the situation is even less clear, with some models predicting increases in chill and others predicting decreases. These findings underscore the importance of model choice, as many models may be arbitrary and can be disregarded, yet uncertainties remain regarding which models accurately represent future conditions. This area of research offers opportunities for further exploration and innovation. 5.12 Chill projection for all of Tunisia The study projected climate change impacts on winter chill for an orchard near Sfax in Central Tunisia, but the region is not the most favorable for temperate fruit and nut tree cultivation. Tunisia is climatically diverse, featuring mountains, plains, coastal areas, and interior deserts, leading to significant variation in historical and future chill availability across the country. Under the leadership of Haifa Benmoussa, the team mapped chill accumulation throughout Tunisia using a framework previously developed. This analysis utilized data from 20 weather stations in Tunisia and neighboring countries. By applying the established analytical framework to each location, they were able to interpolate results and create chill maps that illustrate the trends in chill availability in Tunisia over the past few decades: Chill availability maps for Tunisia for several past scenarios The process of interpolating site-specific results into a comprehensive map for Tunisia involves some areas for improvement. Currently, the methodology uses site-specific predictions of Safe Winter Chill, defined as the 10th percentile of the chill distribution derived from annual temperature dynamics generated by the weather model. This information is then interpolated using the Kriging technique. In addition, the elevations of the locations where chill was modeled are also considered. A linear model is fitted to establish a relationship between chill accumulation and elevation. Using a Digital Elevation Model (DEM), the differences between the model-derived elevations from weather stations and the actual elevations of each location are calculated. This difference, not accounted for in the initial chill surface derived from weather station data, is corrected using the established elevation-chill relationship. While this method seems reasonable for Tunisia, it may not be suitable for cooler regions like Germany, where the relationship between elevation and chill availability may not be linear. The resulting projection of future chill for Tunisia is displayed in the following map: Chill availability for Tunisia for various plausible scenarios of future climate The projections reveal significant concern regarding winter chill in Tunisia. The Dynamic Model, which is regarded as a reliable predictor, indicates substantial decreases in Chill Portions, the units used by the model. This trend poses serious challenges for much of the country. Even in areas where some winter chill is expected to persist, farmers will need to adapt their practices, as the tree species currently cultivated are suited to past climate conditions. Adaptation strategies may include shifting to tree cultivars with lower chilling requirements, provided such options are available. 5.13 Revisiting chill accumulation in Oman After a decade of exploration in other regions, the analysis turned back to Oman, where there was a desire to enhance the initial study of chill accumulation. The first assessment had limitations, particularly concerning model selection and a lack of adequate future climate data. With encouragement from Prof. Dr. Andreas Bürkert, a more robust evaluation became possible using the climate change analysis framework. This involved incorporating new methods to convert daily temperatures into hourly data. Updated assessments of past winter chill and future forecasts for the oases of Al Jabal Al Akhdar were produced, with the findings published in Climatic Change (Buerkert et al., 2020). 5.14 Exercises on past chill projections Sketch out three data access and processing challenges that had to be overcome in order to produce chill projections with state-of-the-art methodology. Accessing Climate Data for Specific Locations: Previous climate datasets like AFRICLIM and ClimateWizard only provided large-scale data. To get weather data for specific locations without downloading too much extra information, an API was created to quickly access data for single sites Converting Daily to Hourly Temperature Data: Chill models need hourly temperature data, but many databases only give daily averages. Early methods for converting daily to hourly data weren’t very good, especially in areas with unique temperatures. Improved algorithms were developed to estimate hourly temperatures more accurately from daily data Handling Large Volumes of Climate Model Outputs: Studying different climate futures involves dealing with a lot of data from many climate models, which can be hard to manage. To handle this large amount of data effectively, workflows were streamlined and selective processing techniques were used Outline, in your understanding, the basic steps that are necessary to make such projections. To make climate-based chill projections for specific regions, here are the essential steps typically involved: Data Collection and Calibration: collect historical weather data and use it to calibrate a weather generator for realistic temperature simulations Model Selection and Scenario Setup: choose relevant climate models and emission scenarios to explore various future climates Generate Temperature Projections: downscale climate data, converting it to daily or hourly temperatures as needed for chill calculations Chill Calculation: apply chill models to estimate chill accumulation across different climate scenarios Analysis and Visualization: compare chill projections across models and scenarios and visualize the findings Interpretation: validate projections with observed data where possible and assess agricultural impacts and adaptation needs "],["manual-chill.html", "Chapter 6 Manual chill 6.1 Learning goals 6.2 Chilling hours calculation using chillR 6.3 Exercises on basic chill modeling", " Chapter 6 Manual chill This chapter explains how to calculate Chilling Hours using R and the chillR package. Chilling Hours measure the number of hours where temperatures are between 0°C and 7.2°C, which is important for certain plants to meet their cold requirements during dormancy and grow properly. 6.1 Learning goals Learn about some basic R operations we need for calculating Chilling Hours Be able to calculate Chilling Hours Understand what an R function is Be able to write your own basic function 6.2 Chilling hours calculation using chillR Basic models like the Chilling Hours Model are simple and can be calculated manually, especially if familiar with R or spreadsheet software. This example will show how to understand and use the functions in the chillR package to calculate these chill hours. 6.2.1 Data Requirements The Chilling Hours Model is relatively simple but requires hourly temperature data. A common challenge is the unavailability of such data, though approximations can be made from daily records using chillR tools. For this example, a sample dataset, Winters_hours_gaps, provided within chillR, was used. It contains hourly temperature data recorded in 2008 from a walnut orchard in Winters, California, and is structured with columns for year, month, day, hour, and temperature. 6.2.2 Loading and Preparing Data To work with chillR, the package was loaded using library(chillR). The data can also be imported via read.table() or read.csv() for external datasets. The Winters_hours_gaps dataset was filtered to retain only the essential columns: year, month, day, hour, and temperature. This cleaned version, stored in a new dataframe called hourtemps, ensures the data is in the correct format for calculating Chilling Hours: #install.packages(&quot;chillR&quot;) library(chillR) library(knitr) library(pander) library(kableExtra) library(tidyverse) hourtemps &lt;- Winters_hours_gaps[,c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;, &quot;Hour&quot;, &quot;Temp&quot;)] Year Month Day Hour Temp 2008 3 3 10 15.127 2008 3 3 11 17.153 2008 3 3 12 18.699 2008 3 3 13 18.699 2008 3 3 14 18.842 2008 3 3 15 19.508 2008 3 3 16 19.318 2008 3 3 17 17.701 2008 3 3 18 15.414 2008 3 3 19 12.727 6.2.3 Manual Calculation of Chilling Hours Chilling Hours are defined as any hour where the temperature falls between 0°C and 7.2°C. A logical comparison was used in R to identify whether each hour met this criterion: hourtemps[, &quot;Chilling_Hour&quot;] &lt;- hourtemps$Temp &gt;= 0 &amp; hourtemps$Temp &lt;= 7.2 Year Month Day Hour Temp Chilling_Hour 2008 3 3 10 15.127 FALSE 2008 3 3 11 17.153 FALSE 2008 3 3 12 18.699 FALSE 2008 3 3 13 18.699 FALSE 2008 3 3 14 18.842 FALSE 2008 3 3 15 19.508 FALSE 2008 3 3 16 19.318 FALSE 2008 3 3 17 17.701 FALSE 2008 3 3 18 15.414 FALSE 2008 3 3 19 12.727 FALSE A new column, Chilling_Hour, was created, indicating whether a given hour was a valid Chilling Hour (TRUE or FALSE). These values can then be summed to calculate the total number of Chilling Hours for any period using the sum() function. 6.2.4 Automation with Functions A function is a tool that automates a particular procedure. It consists of a name, some arguments that are passed to the function, and some code that should be executed. To avoid repeating manual calculations, a reusable function called CH was created to automate the addition of a Chilling_Hour column: CH &lt;- function(hourtemps) { hourtemps[, &quot;Chilling_Hour&quot;] &lt;- hourtemps$Temp &gt;= 0 &amp; hourtemps$Temp &lt;= 7.2 return(hourtemps) } This function applies to any appropriately structured dataset. Additionally, a more complex function, sum_CH, was developed to calculate the total Chilling Hours between two specific dates: sum_CH &lt;- function(hourtemps, Start_Year, Start_Month, Start_Day, Start_Hour, End_Year, End_Month, End_Day, End_Hour) {hourtemps[,&quot;Chilling_Hour&quot;] &lt;- hourtemps$Temp &gt; 0 &amp; hourtemps$Temp &lt;= 7.2 Start_Date &lt;- which(hourtemps$Year == Start_Year &amp; hourtemps$Month == Start_Month &amp; hourtemps$Day == Start_Day &amp; hourtemps$Hour == Start_Hour) End_Date &lt;- which(hourtemps$Year == End_Year &amp; hourtemps$Month == End_Month &amp; hourtemps$Day == End_Day &amp; hourtemps$Hour == End_Hour) CHs &lt;- sum(hourtemps$Chilling_Hour[Start_Date:End_Date]) return(CHs) } This function calculates Chilling Hours for a user-defined date range, using the which() function to identify the relevant rows in the dataset corresponding to the start and end dates. To simplify the parameter passing, compact strings in the format YEARMODAHO (year, month, day, and hour as consecutive numbers) can be used instead of individual parameters for year, month, day, and hour. The start and end times are now passed as strings, from which the required values are extracted using the substr() function and converted to numeric values with as.numeric(). sum_CH &lt;- function(hourtemps, startYEARMODAHO, endYEARMODAHO) {hourtemps[,&quot;Chilling_Hour&quot;] &lt;- hourtemps$Temp &gt; 0 &amp; hourtemps$Temp &lt;= 7.2 startYear &lt;- as.numeric(substr(startYEARMODAHO, 1, 4)) startMonth &lt;- as.numeric(substr(startYEARMODAHO, 5, 6)) startDay &lt;- as.numeric(substr(startYEARMODAHO, 7, 8)) startHour &lt;- as.numeric(substr(startYEARMODAHO, 9, 10)) endYear &lt;- as.numeric(substr(endYEARMODAHO, 1, 4)) endMonth &lt;- as.numeric(substr(endYEARMODAHO, 5, 6)) endDay &lt;- as.numeric(substr(endYEARMODAHO, 7, 8)) endHour &lt;- as.numeric(substr(endYEARMODAHO, 9, 10)) Start_row &lt;- which(hourtemps$Year == startYear &amp; hourtemps$Month == startMonth &amp; hourtemps$Day == startDay &amp; hourtemps$Hour == startHour ) End_row &lt;- which(hourtemps$Year == endYear &amp; hourtemps$Month == endMonth &amp; hourtemps$Day == endDay &amp; hourtemps$Hour == endHour ) CHs &lt;- sum(hourtemps$Chilling_Hour[Start_row:End_row]) return(CHs) } 6.2.5 Application Example The functions created allow for efficient calculation of Chilling Hours. For instance, using the sum_CH() function, it was calculated that between April 1st and October 11th, 2008, the walnut orchard experienced 77 Chilling Hours: sum_CH(hourtemps, startYEARMODAHO =2008040100, endYEARMODAHO = 2008101100) ## [1] 77 6.3 Exercises on basic chill modeling Write a basic function that calculates warm hours (&gt;25°C). WH &lt;- function(data) {data[, &quot;Warm_Hour&quot;] &lt;- data$Temp &gt; 25 return(data) } Apply this function to the Winters_hours_gaps dataset. WH(Winters_hours_gaps) Year Month Day Hour Temp_gaps Temp Warm_Hour 2008 3 3 10 15.127 15.127 FALSE 2008 3 3 11 17.153 17.153 FALSE 2008 3 3 12 18.699 18.699 FALSE 2008 3 3 13 18.699 18.699 FALSE 2008 3 3 14 18.842 18.842 FALSE 2008 3 3 15 19.508 19.508 FALSE 2008 3 3 16 19.318 19.318 FALSE 2008 3 3 17 17.701 17.701 FALSE 2008 3 3 18 15.414 15.414 FALSE 2008 3 3 19 12.727 12.727 FALSE Extend this function, so that it can take start and end dates as inputs and sums up warm hours between these dates. sum_WH &lt;- function(data, startYEARMODAHO, endYEARMODAHO) {data[,&quot;Warm_Hour&quot;] &lt;- data$Temp &gt; 25 startYear &lt;- as.numeric(substr(startYEARMODAHO, 1, 4)) startMonth &lt;- as.numeric(substr(startYEARMODAHO, 5, 6)) startDay &lt;- as.numeric(substr(startYEARMODAHO, 7, 8)) startHour &lt;- as.numeric(substr(startYEARMODAHO, 9, 10)) endYear &lt;- as.numeric(substr(endYEARMODAHO, 1, 4)) endMonth &lt;- as.numeric(substr(endYEARMODAHO, 5, 6)) endDay &lt;- as.numeric(substr(endYEARMODAHO, 7, 8)) endHour &lt;- as.numeric(substr(endYEARMODAHO, 9, 10)) Start_Date &lt;- which(data$Year == startYear &amp; data$Month == startMonth &amp; data$Day == startDay &amp; data$Hour == startHour) End_Date &lt;- which(data$Year == endYear &amp; data$Month == endMonth &amp; data$Day == endDay &amp; data$Hour == endHour) WHs &lt;- sum(data$Warm_Hour[Start_Date:End_Date]) return(WHs) } Application Example: sum_WH(Winters_hours_gaps, startYEARMODAHO = 2008080100, endYEARMODAHO = 2008083100) ## [1] 283 During the month of August 2008, from the 1st to the 31st, the walnut orchard experienced a total of 283 warm hours (defined as hours when the temperature exceeded 25°C). "],["chill.html", "Chapter 7 Chill 7.1 Learning goals 7.2 Chilling_Hours() function 7.3 Utah Model 7.4 Creating Custom Chill Models with step_model() 7.5 Dynamic model 7.6 Chilling and tempResponse functions 7.7 Exercises on chill models", " Chapter 7 Chill In this chapter, various chill models will be explored using the chillR package in R, which simplifies the calculation of chilling hours and other dormancy-related metrics based on temperature data. 7.1 Learning goals Learn how to run chill models using chillR Learn how to produce your own temperature response model in chillR 7.2 Chilling_Hours() function The Chilling_Hours() function calculates the time during which temperatures fall within a key range for chill accumulation. It takes hourly temperature data as input and, by default, provides the cumulative amount of chilling accumulated over time. Chilling_Hours(Winters_hours_gaps$Temp)[1:100] ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 2 3 4 5 6 6 6 6 6 6 6 6 ## [30] 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 ## [59] 6 6 6 7 8 9 10 11 12 13 14 15 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 17 18 ## [88] 19 20 21 22 23 24 25 25 25 25 25 25 25 The result will show the first 100 values, where the cumulative chilling hours increase as the temperature falls within the specified range. 7.3 Utah Model The Utah Model assigns different weights to various temperature ranges, reflecting their impact on chill accumulation. The Utah_Model() function in chillR calculates these weighted chilling contributions for each hour of temperature data. The output will show the Utah model values for the first 100 hours, where positive, zero, and negative weights are applied based on the temperature: Utah_Model(Winters_hours_gaps$Temp)[1:100] ## [1] 0.0 -0.5 -1.5 -2.5 -3.5 -4.5 -5.5 -6.0 -6.0 -6.0 -5.5 -5.0 -4.0 -3.0 -2.0 -1.0 0.0 ## [18] 0.5 1.5 2.5 3.5 4.5 5.0 5.0 5.0 4.0 3.0 2.0 1.0 0.0 -1.0 -2.0 -2.5 -2.5 ## [35] -2.0 -1.5 -1.0 -0.5 0.5 1.0 1.5 2.0 2.0 2.5 3.0 3.5 4.0 4.0 4.0 3.5 2.5 ## [52] 1.5 0.5 -0.5 -1.5 -2.5 -3.0 -3.0 -2.5 -1.5 -0.5 0.5 1.5 2.5 3.5 4.5 5.5 6.5 ## [69] 7.5 8.5 9.5 10.0 10.0 9.5 9.0 8.5 8.0 7.5 7.0 6.5 6.5 7.0 7.5 8.5 9.5 ## [86] 10.5 11.5 12.5 13.5 14.5 15.5 16.5 17.5 18.5 19.0 19.0 19.0 18.5 17.5 16.5 7.4 Creating Custom Chill Models with step_model() The step_model() function, part of the chillR package, enables the creation of custom chill models based on temperature thresholds and weights. This process involves defining a data frame that specifies temperature ranges and their corresponding weights. Here’s an example of a data frame that defines temperature ranges and their corresponding weights: df &lt;- data.frame( lower = c(-1000, 1, 2, 3, 4, 5, 6), upper = c( 1, 2, 3, 4, 5, 6, 1000), weight = c( 0, 1, 2, 3, 2, 1, 0)) lower upper weight -1000 1 0 1 2 1 2 3 2 3 4 3 4 5 2 5 6 1 6 1000 0 A function called custom() implements a chill model based on this data frame. This function is then applied to the Winters_hours_gaps dataset to calculate the chilling contributions: custom &lt;- function(x) step_model(x, df) custom(Winters_hours_gaps$Temp)[1:100] ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 4 7 7 7 7 7 7 7 7 ## [30] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ## [59] 7 7 7 7 7 7 8 10 13 16 19 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 ## [88] 23 25 27 29 31 34 37 37 37 37 37 37 37 7.5 Dynamic model The Dynamic Model provides a more complex and reliable approach to calculating chill, with the Dynamic_Model() function handling the intricate equations involved. This function can be easily applied to the Winters_hours_gaps dataset, producing output that displays dynamic chill values for the first 100 hours, reflecting the underlying physiological processes: Dynamic_Model(Winters_hours_gaps$Temp)[1:100] ## [1] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 ## [9] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 ## [17] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 ## [25] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 ## [33] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 ## [41] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 ## [49] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 ## [57] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 ## [65] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.9698435 ## [73] 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 ## [81] 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 ## [89] 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 0.9698435 ## [97] 0.9698435 0.9698435 0.9698435 0.9698435 7.6 Chilling and tempResponse functions The chillR package offers several functions for analyzing hourly temperature data, including wrapper functions that enable the computation of chill between specific start and end dates. The chilling() function automatically calculates various basic metrics, including Chilling Hours, Utah Model, Dynamic Model, and Growing Degree Hours. It is important to use the make_JDay() function to add Julian dates (which count the days of the year) to the dataset, ensuring proper functionality. chill_output &lt;- chilling(make_JDay(Winters_hours_gaps), Start_JDay = 90, End_JDay = 100) Season End_year Season_days Data_days Perc_complete Chilling_Hours Utah_Model Chill_portions GDH 2007/2008 2008 11 11 100 40 15.5 2.009147 2406.52 However, there may be instances where not all metrics are desired, or there is a need for different metrics altogether. In such cases, the tempResponse function can be employed. This function is similar to chilling() but offers the flexibility to take a list of specific temperature models to be computed as input. chill_output &lt;- tempResponse(make_JDay(Winters_hours_gaps), Start_JDay = 90, End_JDay = 100, models = list(Chill_Portions = Dynamic_Model, GDH = GDH)) Season End_year Season_days Data_days Perc_complete Chill_Portions GDH 2007/2008 2008 11 11 100 2.009147 2406.52 This will return only the Dynamic Model and Growing Degree Hours (GDH) values for the specified period. 7.7 Exercises on chill models Run the chilling() function on the Winters_hours_gap dataset. august &lt;- chilling(make_JDay(Winters_hours_gaps), Start_JDay = 214, End_JDay = 244) Season End_year Season_days Data_days Perc_complete Chilling_Hours Utah_Model Chill_portions GDH 2007/2008 2008 31 31 100 0 -569.5 0 9933.327 Create your own temperature-weighting chill model using the step_model() function. df &lt;- data.frame( lower = c(-1000, 0, 5, 10, 15, 20, 25), upper = c( 0, 5, 10, 15, 20, 25, 1000), weight = c( 0, 1, 2, 3, 2, 1, 0)) custom &lt;- function(x) step_model(x, df) lower upper weight -1000 0 0 0 5 1 5 10 2 10 15 3 15 20 2 20 25 1 25 1000 0 Run this model on the Winters_hours_gaps dataset using the tempResponse() function. models &lt;- list( Chilling_Hours = Chilling_Hours, Utah_Chill_Units = Utah_Model, Chill_Portions = Dynamic_Model, GDH = GDH, custom = custom) result &lt;- tempResponse(make_JDay(Winters_hours_gaps), Start_JDay = 214, End_JDay = 244, models) Season End_year Season_days Data_days Perc_complete Chilling_Hours Utah_Chill_Units Chill_Portions GDH custom 2007/2008 2008 31 31 100 0 -569.5 0 9933.327 838 "],["making-hourly-temperatures.html", "Chapter 8 Making hourly temperatures 8.1 Learning goals 8.2 Generating hourly temperatures 8.3 Idealized daily temperature curves 8.4 Empirical daily temperature curves 8.5 Exercises on hourly temperatures", " Chapter 8 Making hourly temperatures 8.1 Learning goals Understand why we often need hourly temperature data and why we need ways of making them out of daily data Understand some basic algorithms for making hourly data from daily minimum and maximum temperatures Understand how we can make use of observed hourly temperatures to produce our own empirical transfer function that can make hourly from daily data Be able to use the respective chillR functions that implement these steps 8.2 Generating hourly temperatures With the Chilling Hours function developed, the next challenge arises from the limited availability of hourly temperature data, as most sources provide only daily minimum and maximum temperatures. This limitation complicates the direct calculation of Chilling Hours. Various methods have been employed to address this issue, including relating Chilling Hours to minimum temperatures (Crossa-Raynaud, 1955) or using complex equations. With better computing tools, some researchers started to assume that daily minimum and maximum temperatures occur at specific times and used linear interpolation for the hours in between, creating a triangular shape for daily temperature patterns (Baldocchi &amp; Wong, 2008). While assuming a triangular temperature pattern may serve as a rough approximation, it is not entirely realistic. The rate of temperature increase in the morning differs from the rate of decrease in the evening. Additionally, the timing of the lowest daily temperature varies significantly throughout the year, particularly outside of equatorial regions. Therefore, it is important to take these variations into account. 8.3 Idealized daily temperature curves A major breakthrough in modeling daily temperature curves was made when Dale E. Linvill from Clemson University published a paper in 1990. He combined two mathematical equations: a sine curve to represent warming during the day and a logarithmic decay function for cooling at night. The times for sunrise and sunset defined the transition between these phases, and the length of each phase was linked to the amount of daylight. This method allowed for more accurate daily temperature curves than previous approaches, but not all researchers adopted these equations due to a lack of awareness or data processing skills. One challenge with Linvill’s equations was their dependence on local sunrise and sunset times. While these can be calculated from observations, having a general method would help researchers. Fortunately, for areas without major geographical features, sunrise and sunset times can be calculated based on solar system geometry. Although agricultural scientists may not be familiar with this, they can use insights from other fields. The chillR package uses equations from Spencer (1971) and Almorox et al. (2005). Prof. Dr. Eike Lüdeling only needed to understand these equations once to code them into an R function for future use. Bringing together these functions is similar to how the CH() function was developed and used in the sum_CH function, though the components were more complex. The result is a function that can create realistic daily temperature curves based on the latitude of a location. The provided code illustrates how to use the daylength function to create plots showing sunrise time, sunset time, and daylength for Klein-Altendorf (Latitude: 50.4°N) throughout the year: Days &lt;- daylength(latitude = 50.4, JDay = 1:365) Days_df &lt;- data.frame( JDay = 1:365, Sunrise = Days$Sunrise, Sunset = Days$Sunset, Daylength = Days$Daylength ) Days_df &lt;- pivot_longer(Days_df, cols = c(Sunrise:Daylength)) ggplot(Days_df, aes(JDay, value)) + geom_line(lwd = 1.5) + facet_grid(cols = vars(name)) + ylab(&quot;Time of Day / Daylength (Hours)&quot;) + theme_bw(base_size = 15) In this context, JDay refers to the Julian Date, which represents the Day of the Year. For example, January 1st is JDay 1, while December 31st is JDay 365 in regular years and JDay 366 in leap years. The ggplot2 package is used for creating attractive plots, and the ideal input for it is a data frame. Therefore, the outputs from the daylength() function were first converted into a data frame. Additionally, the three time series - Sunrise, Sunset, and Daylength - were organized into a stacked format using the pivot_longer command from the tidyr package. The stack_hourly_temps() function in the chillR package integrates these daily dynamics. This function requires a dataset containing daily minimum and maximum temperatures, specifically with columns labeled Tmin, Tmax, Year, Month, and Day. The latitude of the location must also be provided. Using these inputs, the function applies the previously discussed equations to calculate hourly temperatures, and it can also output sunrise and sunset times if desired. To demonstrate this process, another dataset included with chillR, called KA_weather, will be used. This data frame contains temperature data from the University of Bonn’s experimental station at Klein-Altendorf. The first 10 rows of the KA_weather dataset will be shown for illustration: Year Month Day Tmax Tmin 1998 1 1 8.2 5.1 1998 1 2 9.1 5.0 1998 1 3 10.4 3.3 1998 1 4 8.4 4.5 1998 1 5 7.7 4.5 1998 1 6 8.1 4.4 1998 1 7 12.0 6.9 1998 1 8 11.2 8.6 1998 1 9 13.9 8.5 1998 1 10 14.5 3.6 The following process describes how hourly temperatures can be calculated based on the idealized daily temperature curve: stack_hourly_temps(KA_weather, latitude = 50.4) Year Month Day Tmax Tmin JDay Hour Temp 1998 1 5 7.7 4.5 5 3 4.844164 1998 1 5 7.7 4.5 5 4 4.746566 1998 1 5 7.7 4.5 5 5 4.656244 1998 1 5 7.7 4.5 5 6 4.572187 1998 1 5 7.7 4.5 5 7 4.493583 1998 1 5 7.7 4.5 5 8 4.569464 1998 1 5 7.7 4.5 5 9 5.384001 1998 1 5 7.7 4.5 5 10 6.139939 1998 1 5 7.7 4.5 5 11 6.787169 1998 1 5 7.7 4.5 5 12 7.282787 1998 1 5 7.7 4.5 5 13 7.593939 1998 1 5 7.7 4.5 5 14 7.700000 1998 1 5 7.7 4.5 5 15 7.593939 1998 1 5 7.7 4.5 5 16 7.282787 1998 1 5 7.7 4.5 5 17 6.591821 1998 1 5 7.7 4.5 5 18 6.168074 1998 1 5 7.7 4.5 5 19 5.870570 1998 1 5 7.7 4.5 5 20 5.641106 1998 1 5 7.7 4.5 5 21 5.454280 1998 1 5 7.7 4.5 5 22 5.296704 1998 1 5 7.7 4.5 5 23 5.160445 And here’s a plot of the data: 8.4 Empirical daily temperature curves In some locations, idealized daily temperature curves are ineffective, particularly in areas with rugged topography where temperate fruit trees may be shaded for part of the day. For example, in the Jabal Al Akhdar region of Oman, where Prof. Dr. Eike Lüdeling initially studied winter chill, various oases in the deeply incised Wadi Muaydin canyon were investigated. Trees near the top of the canyon receive significantly more sunlight than those at the bottom, which is about 1000 meters lower. Terraced oasis fields at Ash Sharayjah Even in the absence of mountains, the temperature curve in an orchard may not closely resemble the curve proposed by Linvill (1990) due to its unique microclimate, featuring shaded and sunny spots, dew-covered grass, and bare ground. In the initial study on Omani oases (Luedeling et al., 2009b), this issue was not adequately addressed. However, a recent revisit to the location aimed to improve this aspect (Buerkert et al., 2020). To analyze the temperature patterns, a dataset of hourly temperature data for the relevant location is needed, ideally covering an entire year or multiple years. For this exercise, the Winters_hours_gaps dataset from a walnut orchard near Winters, California, will be used, as the temperature logger was directly attached to a tree branch, making it unlikely for the data to exactly match the standard daily temperature curve. The Empirical_daily_temperature_curve() function will be employed to determine the typical hourly temperature patterns for each month of the year, although this method could potentially be enhanced by allowing for continuous analysis instead of monthly breakdowns. empi_curve &lt;- Empirical_daily_temperature_curve(Winters_hours_gaps) Month Hour Prediction_coefficient 3 0 0.1774859 3 1 0.1550693 3 2 0.1285651 3 3 0.1145597 3 4 0.0696064 3 5 0.0339583 3 6 0.0000000 3 7 0.0313115 3 8 0.3121959 3 9 0.4953232 3 10 0.6819674 3 11 0.8227423 3 12 0.9506491 3 13 0.9662604 3 14 0.9915996 3 15 1.0000000 3 16 0.9490319 3 17 0.8483098 3 18 0.6864529 3 19 0.4945415 3 20 0.3636642 3 21 0.2972377 3 22 0.2360349 3 23 0.1794802 4 0 0.1960789 4 1 0.1407018 4 2 0.1283250 4 3 0.0819307 4 4 0.0541415 4 5 0.0188241 4 6 0.0000000 4 7 0.1697052 4 8 0.4442722 4 9 0.5939797 4 10 0.7363923 4 11 0.8399804 4 12 0.9245702 4 13 0.9770693 4 14 0.9963131 4 15 1.0000000 4 16 0.9568107 4 17 0.8698369 4 18 0.7343896 4 19 0.5330597 4 20 0.3941038 4 21 0.3186075 4 22 0.2594569 4 23 0.2114486 ggplot(data = empi_curve[1:96, ], aes(Hour, Prediction_coefficient)) + geom_line(lwd = 1.3, col = &quot;red&quot;) + facet_grid(rows = vars(Month)) + xlab(&quot;Hour of the day&quot;) + ylab(&quot;Prediction coefficient&quot;) + theme_bw(base_size = 15) The set of coefficients can now be applied to a daily dataset from the same location, allowing for the creation of a reasonable hourly temperature record for the orchard. This is accomplished using the Empirical_hourly_temperatures function, which requires a set of hourly coefficients and a daily temperature record that includes Tmin and Tmax columns. Additionally, the ? operator can be used to access help on how to use any function, such as ?Empirical_hourly_temperatures. The process also involves using the make_all_day_table function, which fills gaps in daily or hourly temperature records and summarizes hourly data into daily minimum and maximum temperatures. coeffs &lt;- Empirical_daily_temperature_curve(Winters_hours_gaps) Winters_daily &lt;- make_all_day_table(Winters_hours_gaps, input_timestep = &quot;hour&quot;) Winters_hours &lt;- Empirical_hourly_temperatures(Winters_daily, coeffs) The next step is to plot the results to visualize the hourly temperature data. This allows for a comparison between the results from the empirical method, the triangular function, and the idealized temperature curve. Additionally, actual observed temperatures will be used to validate the results. To facilitate this process, the data will first be simplified for easier handling: Winters_hours &lt;- Winters_hours[, c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;, &quot;Hour&quot;, &quot;Temp&quot;)] colnames(Winters_hours)[ncol(Winters_hours)] &lt;- &quot;Temp_empirical&quot; Winters_ideal &lt;- stack_hourly_temps(Winters_daily, latitude = 38.5)$hourtemps Winters_ideal &lt;- Winters_ideal[, c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;, &quot;Hour&quot;, &quot;Temp&quot;)] colnames(Winters_ideal)[ncol(Winters_ideal)] &lt;- &quot;Temp_ideal&quot; The next step involves creating the ‘triangular’ dataset. Understanding the process behind this construction is essential. Winters_triangle &lt;- Winters_daily Winters_triangle[, &quot;Hour&quot;] &lt;- 0 Winters_triangle$Hour[nrow(Winters_triangle)] &lt;- 23 Winters_triangle[, &quot;Temp&quot;] &lt;- 0 Winters_triangle &lt;- make_all_day_table(Winters_triangle, timestep = &quot;hour&quot;) colnames(Winters_triangle)[ncol(Winters_triangle)] &lt;- &quot;Temp_triangular&quot; # with the following loop, we fill in the daily Tmin and Tmax values for every # hour of the dataset for (i in 2:nrow(Winters_triangle)) { if (is.na(Winters_triangle$Tmin[i])) Winters_triangle$Tmin[i] &lt;- Winters_triangle$Tmin[i - 1] if (is.na(Winters_triangle$Tmax[i])) Winters_triangle$Tmax[i] &lt;- Winters_triangle$Tmax[i - 1] } Winters_triangle$Temp_triangular &lt;- NA # now we assign the daily Tmin value to the 6th hour of every day Winters_triangle$Temp_triangular[which(Winters_triangle$Hour == 6)] &lt;- Winters_triangle$Tmin[which(Winters_triangle$Hour == 6)] # we also assign the daily Tmax value to the 18th hour of every day Winters_triangle$Temp_triangular[which(Winters_triangle$Hour == 18)] &lt;- Winters_triangle$Tmax[which(Winters_triangle$Hour == 18)] # in the following step, we use the chillR function &quot;interpolate_gaps&quot; # to fill in all the gaps in the hourly record with straight lines Winters_triangle$Temp_triangular &lt;- interpolate_gaps(Winters_triangle$Temp_triangular)$interp Winters_triangle &lt;- Winters_triangle[, c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;, &quot;Hour&quot;, &quot;Temp_triangular&quot;)] The next step is to merge all the data frames to facilitate easier display and comparison of the datasets. Winters_temps &lt;- merge(Winters_hours_gaps, Winters_hours, by = c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;, &quot;Hour&quot;)) Winters_temps &lt;- merge(Winters_temps, Winters_triangle, by = c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;, &quot;Hour&quot;)) Winters_temps &lt;- merge(Winters_temps, Winters_ideal, by = c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;, &quot;Hour&quot;)) The dataset now includes observed temperatures along with the three approximations: triangular, idealized curve, and empirical curve. To plot this data effectively, the Year, Month, Day, and Hour columns will be converted into R’s date format using ISOdate, and the data frame will be reorganized for better usability. Winters_temps[, &quot;DATE&quot;] &lt;- ISOdate(Winters_temps$Year, Winters_temps$Month, Winters_temps$Day, Winters_temps$Hour) Winters_temps_to_plot &lt;- Winters_temps[, c(&quot;DATE&quot;, &quot;Temp&quot;, &quot;Temp_empirical&quot;, &quot;Temp_triangular&quot;, &quot;Temp_ideal&quot;)] Winters_temps_to_plot &lt;- Winters_temps_to_plot[100:200, ] Winters_temps_to_plot &lt;- pivot_longer(Winters_temps_to_plot, cols=Temp:Temp_ideal) colnames(Winters_temps_to_plot) &lt;- c(&quot;DATE&quot;, &quot;Method&quot;, &quot;Temperature&quot;) ggplot(data = Winters_temps_to_plot, aes(DATE, Temperature, colour = Method)) + geom_line(lwd = 1.3) + ylab(&quot;Temperature (°C)&quot;) + xlab(&quot;Date&quot;) The plot indicates that the triangular curve deviates significantly from the observed data, while the Temp_empirical and Temp_ideal curves appear quite similar and are difficult to differentiate. To compare these curves more thoroughly, the Root Mean Square Error (RMSE) can be calculated, as it is useful for quantifying the alignment between predicted and observed values. The chillR package includes a function to perform this calculation. # here&#39;s the RMSE for the triangular method: RMSEP(Winters_temps$Temp_triangular, Winters_temps$Temp) ## [1] 4.695289 # here&#39;s the RMSE for the idealized-curve method: RMSEP(Winters_temps$Temp_ideal, Winters_temps$Temp) ## [1] 1.630714 # and here&#39;s the RMSE for the empirical-curve method: RMSEP(Winters_temps$Temp_empirical, Winters_temps$Temp) ## [1] 1.410625 The results show an RMSE of 4.7 for the triangular method, 1.63 for the idealized curve method, and 1.41 for the empirical curve method. Since a lower RMSE indicates better accuracy, these results demonstrate that calibrating the prediction function with observed hourly data significantly improves accuracy, especially compared to the triangular method. While it might be questioned how much this affects chill accumulation modeling, it is often found to make a considerable difference. 8.5 Exercises on hourly temperatures Choose a location of interest, find out its latitude and produce plots of daily sunrise, sunset and daylength. The Yakima Valley in Washington State, USA, is located at about 46.6° N latitude. This region has a continental climate with cold winters and hot, dry summers, creating ideal conditions for growing fruit trees. The valley is well known for producing a variety of fruits, including apples, cherries, pears, and grapes, which benefit from its distinct seasonal changes. Using the daylength() function, you could create plots showing daily sunrise, sunset, and day length times. Yakima &lt;- daylength(latitude = 46.6, JDay = 1:365) Yakima_df &lt;- data.frame( JDay = 1:365, Sunrise = Yakima$Sunrise, Sunset = Yakima$Sunset, Daylength = Yakima$Daylength ) Yakima_df_longer &lt;- pivot_longer(Yakima_df, cols = c(Sunrise:Daylength)) ggplot(Yakima_df_longer, aes(JDay, value)) + geom_line(lwd = 1.5) + facet_grid(cols = vars(name)) + ylab(&quot;Time of Day / Daylength (Hours)&quot;) + theme_bw(base_size = 15) Produce an hourly dataset, based on idealized daily curves, for the KA_weather dataset (included in chillR) KA_hourly &lt;- stack_hourly_temps(KA_weather, latitude = 50.4) Based on idealized daily curves, the hourly dataset for Julian Day 6 (January 6th) is shown below: Year Month Day Tmax Tmin JDay Hour Temp 1998 1 6 8.1 4.4 6 0 4.990741 1998 1 6 8.1 4.4 6 1 4.881232 1998 1 6 8.1 4.4 6 2 4.782253 1998 1 6 8.1 4.4 6 3 4.691956 1998 1 6 8.1 4.4 6 4 4.608939 1998 1 6 8.1 4.4 6 5 4.532117 1998 1 6 8.1 4.4 6 6 4.460628 1998 1 6 8.1 4.4 6 7 4.393780 1998 1 6 8.1 4.4 6 8 4.491337 1998 1 6 8.1 4.4 6 9 5.430950 1998 1 6 8.1 4.4 6 10 6.302486 1998 1 6 8.1 4.4 6 11 7.048391 1998 1 6 8.1 4.4 6 12 7.619410 1998 1 6 8.1 4.4 6 13 7.977836 1998 1 6 8.1 4.4 6 14 8.100000 1998 1 6 8.1 4.4 6 15 7.977836 1998 1 6 8.1 4.4 6 16 7.619410 1998 1 6 8.1 4.4 6 17 7.419674 1998 1 6 8.1 4.4 6 18 7.318918 1998 1 6 8.1 4.4 6 19 7.248287 1998 1 6 8.1 4.4 6 20 7.193854 1998 1 6 8.1 4.4 6 21 7.149557 1998 1 6 8.1 4.4 6 22 7.112208 1998 1 6 8.1 4.4 6 23 7.079920 Produce empirical temperature curve parameters for the Winters_hours_gaps dataset, and use them to predict hourly values from daily temperatures (this is very similar to the example above, but please make sure you understand what’s going on). # Generating empirical daily temperature curve from observed hourly data empi_curve &lt;- Empirical_daily_temperature_curve(Winters_hours_gaps) # Filling gaps in daily or hourly temperature data Winters_daily &lt;- make_all_day_table(Winters_hours_gaps, input_timestep = &quot;hour&quot;) # Using empirical coefficients to predict hourly temperatures based on daily temperatures Winters_hours &lt;- Empirical_hourly_temperatures(Winters_daily, empi_curve) # Make an empirical dataset Winters_hours &lt;- Winters_hours[, c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;, &quot;Hour&quot;, &quot;Temp&quot;)] colnames(Winters_hours)[ncol(Winters_hours)] &lt;- &quot;Temp_empirical&quot; # Merge data frames Winters_temps &lt;- merge(Winters_hours_gaps, Winters_hours, by = c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;, &quot;Hour&quot;)) # Covert Year, Month, Day and Hour columns into R&#39;s date formate and reorganizing the data frame Winters_temps[, &quot;DATE&quot;] &lt;- ISOdate(Winters_temps$Year, Winters_temps$Month, Winters_temps$Day, Winters_temps$Hour) Winters_temps_to_plot &lt;- Winters_temps[, c(&quot;DATE&quot;, &quot;Temp&quot;, &quot;Temp_empirical&quot;)] Winters_temps_to_plot &lt;- Winters_temps_to_plot[100:200, ] Winters_temps_to_plot &lt;- pivot_longer(Winters_temps_to_plot, cols=Temp:Temp_empirical) colnames(Winters_temps_to_plot) &lt;- c(&quot;DATE&quot;, &quot;Method&quot;, &quot;Temperature&quot;) ggplot(data = Winters_temps_to_plot, aes(DATE, Temperature, colour = Method)) + geom_line(lwd = 1.3) + ylab(&quot;Temperature (°C)&quot;) + xlab(&quot;Date&quot;) "],["some-useful-tools-in-r.html", "Chapter 9 Some useful tools in R 9.1 Learning goals 9.2 An evolving language - and a lifelong learning process 9.3 The tidyverse 9.4 The ggplot2 package 9.5 The tibble package 9.6 The magrittr package - pipes 9.7 The tidyr package 9.8 Loops 9.9 apply functions 9.10 Exercises on useful R tools", " Chapter 9 Some useful tools in R 9.1 Learning goals Get to know some neat tools in R that can make coding more elegant - and easier Get introduced to the tidyverse Learn about loops Get to know the apply function family 9.2 An evolving language - and a lifelong learning process The R universe is constantly evolving, offering much more now than the original base functions. Over time, modern tools and more elegant programming styles have become integral. In the upcoming chapters, some of these new tools will be introduced, along with the basics needed to use them effectively. 9.3 The tidyverse Many of the tools introduced here come from the tidyverse - a collection of packages developed by Hadley Wickham and his team. This collection offers many ways to improve programming skills. In this book, only the functions that are directly used will be covered. A big advantage of the tidyverse is that, with just one command - library(tidyverse) - all functions in the package collection become available. 9.4 The ggplot2 package The ggplot2 package, first released by Hadley Wickham in 2007, has become one of the most popular R packages because it greatly simplifies the creation of attractive graphics. The history of the package can be found here, and an introduction along with links to various tutorials is available here. 9.5 The tibble package A tibble is an enhanced version of a data.frame that offers several improvements. The most notable improvement is that tibbles avoid the common data.frame behavior of unexpectedly converting strings into factors. Although tibbles are relatively new here, they will be used throughout the rest of the book. To create a tibble from a regular data.frame (or a similar structure), the as_tibble command can be used: dat &lt;- data.frame(a = c(1, 2, 3), b = c(4, 5, 6)) d &lt;- as_tibble(dat) d ## # A tibble: 3 × 2 ## a b ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 ## 2 2 5 ## 3 3 6 9.6 The magrittr package - pipes Magrittr helps organize steps applied to the same dataset by using the pipe operator %&gt;%. This operator links multiple operations on a data structure, such as a tlibbe, making it easier to perform tasks like calculating the sum of all numbers in the dataset: d %&gt;% sum() ## [1] 21 After the pipe operator %&gt;%, the next function automatically takes the piped-in data as its first input, so it’s unnecessary to specify it explicitly. Additional commands can be chained by adding more pipes. This approach allows for building more complex workflows, as shown in examples later. 9.7 The tidyr package The tidyr package offers helpful functions for organizing data. The KA_weather dataset from chillR will be used here to illustrate some of these functions: KAw &lt;- as_tibble(KA_weather[1:10,]) KAw ## # A tibble: 10 × 5 ## Year Month Day Tmax Tmin ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998 1 1 8.2 5.1 ## 2 1998 1 2 9.1 5 ## 3 1998 1 3 10.4 3.3 ## 4 1998 1 4 8.4 4.5 ## 5 1998 1 5 7.7 4.5 ## 6 1998 1 6 8.1 4.4 ## 7 1998 1 7 12 6.9 ## 8 1998 1 8 11.2 8.6 ## 9 1998 1 9 13.9 8.5 ## 10 1998 1 10 14.5 3.6 9.7.1 pivot_longer The pivot_longer function, introduced previously, is useful for reshaping data from separate columns (like Tmin and Tmax) into individual rows. In this setup, each day’s record will have a row for Tmin and another for Tmax. This transformation is often necessary for tasks like plotting data with the ggplot2 package. The function can be combined with a pipe for a streamlined workflow: KAwlong &lt;- KAw %&gt;% pivot_longer(cols = Tmax:Tmin) KAwlong ## # A tibble: 20 × 5 ## Year Month Day name value ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1998 1 1 Tmax 8.2 ## 2 1998 1 1 Tmin 5.1 ## 3 1998 1 2 Tmax 9.1 ## 4 1998 1 2 Tmin 5 ## 5 1998 1 3 Tmax 10.4 ## 6 1998 1 3 Tmin 3.3 ## 7 1998 1 4 Tmax 8.4 ## 8 1998 1 4 Tmin 4.5 ## 9 1998 1 5 Tmax 7.7 ## 10 1998 1 5 Tmin 4.5 ## 11 1998 1 6 Tmax 8.1 ## 12 1998 1 6 Tmin 4.4 ## 13 1998 1 7 Tmax 12 ## 14 1998 1 7 Tmin 6.9 ## 15 1998 1 8 Tmax 11.2 ## 16 1998 1 8 Tmin 8.6 ## 17 1998 1 9 Tmax 13.9 ## 18 1998 1 9 Tmin 8.5 ## 19 1998 1 10 Tmax 14.5 ## 20 1998 1 10 Tmin 3.6 In this example, it was necessary to specify the columns to stack. The pivot_longer function serves a similar purpose to the melt function from the reshape2 package, which was used previously and in earlier book editions. However, pivot_longer is more intuitive, so it will be used throughout the remaining chapters. 9.7.2 pivot_wider The pivot_wider function allows for the opposite transformation of pivot_longer, converting rows back into separate columns: KAwwide &lt;- KAwlong %&gt;% pivot_wider(names_from = name) KAwwide ## # A tibble: 10 × 5 ## Year Month Day Tmax Tmin ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998 1 1 8.2 5.1 ## 2 1998 1 2 9.1 5 ## 3 1998 1 3 10.4 3.3 ## 4 1998 1 4 8.4 4.5 ## 5 1998 1 5 7.7 4.5 ## 6 1998 1 6 8.1 4.4 ## 7 1998 1 7 12 6.9 ## 8 1998 1 8 11.2 8.6 ## 9 1998 1 9 13.9 8.5 ## 10 1998 1 10 14.5 3.6 The names_from argument in pivot_wider specifies the column that will provide the headers for the new columns. In some cases, pivot_wider might work without this argument, but it’s generally recommended to include it for clarity and to ensure that the function behaves as expected, especially with more complex datasets. 9.7.3 select The select function allows users to choose a subset of columns from a data.frame or tibble: KAw %&gt;% select(c(Month, Day, Tmax)) ## # A tibble: 10 × 3 ## Month Day Tmax ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 8.2 ## 2 1 2 9.1 ## 3 1 3 10.4 ## 4 1 4 8.4 ## 5 1 5 7.7 ## 6 1 6 8.1 ## 7 1 7 12 ## 8 1 8 11.2 ## 9 1 9 13.9 ## 10 1 10 14.5 9.7.4 filter The filter function reduces a data.frame or tibble to just the rows that fulfill certain conditions: KAw %&gt;% filter(Tmax &gt; 10) ## # A tibble: 5 × 5 ## Year Month Day Tmax Tmin ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998 1 3 10.4 3.3 ## 2 1998 1 7 12 6.9 ## 3 1998 1 8 11.2 8.6 ## 4 1998 1 9 13.9 8.5 ## 5 1998 1 10 14.5 3.6 9.7.5 mutate The mutate function is essential for creating, modifying, and deleting columns in a data.frame or tibble. For example, it can be used to add new columns, such as converting Tmin and Tmax to Kelvin: KAw_K &lt;- KAw %&gt;% mutate(Tmax_K = Tmax + 273.15, Tmin_K = Tmin + 273.15) KAw_K ## # A tibble: 10 × 7 ## Year Month Day Tmax Tmin Tmax_K Tmin_K ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998 1 1 8.2 5.1 281. 278. ## 2 1998 1 2 9.1 5 282. 278. ## 3 1998 1 3 10.4 3.3 284. 276. ## 4 1998 1 4 8.4 4.5 282. 278. ## 5 1998 1 5 7.7 4.5 281. 278. ## 6 1998 1 6 8.1 4.4 281. 278. ## 7 1998 1 7 12 6.9 285. 280. ## 8 1998 1 8 11.2 8.6 284. 282. ## 9 1998 1 9 13.9 8.5 287. 282. ## 10 1998 1 10 14.5 3.6 288. 277. To delete the columns created with mutate, you can set them to NULL. This effectively removes the specified columns from the data.frame or tibble: KAw_K %&gt;% mutate(Tmin_K = NULL, Tmax_K = NULL) ## # A tibble: 10 × 5 ## Year Month Day Tmax Tmin ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998 1 1 8.2 5.1 ## 2 1998 1 2 9.1 5 ## 3 1998 1 3 10.4 3.3 ## 4 1998 1 4 8.4 4.5 ## 5 1998 1 5 7.7 4.5 ## 6 1998 1 6 8.1 4.4 ## 7 1998 1 7 12 6.9 ## 8 1998 1 8 11.2 8.6 ## 9 1998 1 9 13.9 8.5 ## 10 1998 1 10 14.5 3.6 Next, the original temperature values will be replaced directly with their corresponding Kelvin values. The following code will make these modifications to the specified columns: KAw %&gt;% mutate(Tmin = Tmin + 273.15, Tmax = Tmax + 273.15) ## # A tibble: 10 × 5 ## Year Month Day Tmax Tmin ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998 1 1 281. 278. ## 2 1998 1 2 282. 278. ## 3 1998 1 3 284. 276. ## 4 1998 1 4 282. 278. ## 5 1998 1 5 281. 278. ## 6 1998 1 6 281. 278. ## 7 1998 1 7 285. 280. ## 8 1998 1 8 284. 282. ## 9 1998 1 9 287. 282. ## 10 1998 1 10 288. 277. 9.7.6 arrange arrange is a function to sort data in data.frames or tibbles: KAw %&gt;% arrange(Tmax, Tmin) ## # A tibble: 10 × 5 ## Year Month Day Tmax Tmin ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998 1 5 7.7 4.5 ## 2 1998 1 6 8.1 4.4 ## 3 1998 1 1 8.2 5.1 ## 4 1998 1 4 8.4 4.5 ## 5 1998 1 2 9.1 5 ## 6 1998 1 3 10.4 3.3 ## 7 1998 1 8 11.2 8.6 ## 8 1998 1 7 12 6.9 ## 9 1998 1 9 13.9 8.5 ## 10 1998 1 10 14.5 3.6 It is also possible to sort a data.frame or tibble in descending order: KAw %&gt;% arrange(desc(Tmax), Tmin) ## # A tibble: 10 × 5 ## Year Month Day Tmax Tmin ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998 1 10 14.5 3.6 ## 2 1998 1 9 13.9 8.5 ## 3 1998 1 7 12 6.9 ## 4 1998 1 8 11.2 8.6 ## 5 1998 1 3 10.4 3.3 ## 6 1998 1 2 9.1 5 ## 7 1998 1 4 8.4 4.5 ## 8 1998 1 1 8.2 5.1 ## 9 1998 1 6 8.1 4.4 ## 10 1998 1 5 7.7 4.5 9.8 Loops In addition to the tidyverse functions, understanding loops is essential for efficient coding. Loops enable the repetition of operations multiple times without needing to retype or copy and paste code. They also allow for modifications to be made with each iteration. While detailed explanations on loops can be found elsewhere, the basics will be covered here. There are two primary types of loops: for loops and while loops. For both types, it is necessary to provide instructions that determine how many times the loop will run, as well as what actions to perform during each iteration. 9.8.1 For loops In a for loop, explicit instructions dictate how many times the code inside the loop should be executed. This is typically done by providing a vector or list of elements, directing R to run the code for each of these elements. As a result, the number of executions corresponds to the number of elements in the vector or list. A counter is needed to track the current iteration, commonly referred to as i, though it can be any variable name: for (i in 1:3) print(&quot;Hello&quot;) ## [1] &quot;Hello&quot; ## [1] &quot;Hello&quot; ## [1] &quot;Hello&quot; This command executed the code three times, producing the same output with each iteration. The structure can be made more complex by including multiple lines of code within curly brackets: addition &lt;- 1 for (i in 1:3) { addition &lt;- addition + 1 print(addition) } ## [1] 2 ## [1] 3 ## [1] 4 The code in this loop incremented an initial value of 1 by 1 in each iteration and printed the resulting value. It is important to note that R may require explicit instructions to print these values when the operation is embedded within a loop. By utilizing the index i within the code block, additional flexibility can be introduced to the operations performed in each iteration: addition &lt;- 1 for (i in 1:3) { addition &lt;- addition + i print(addition) } ## [1] 2 ## [1] 4 ## [1] 7 In this iteration, the respective value of i was added to the initial element during each run. Additionally, i can be utilized in more creative ways within the loop to enhance the operations being performed: names &lt;- c(&quot;Paul&quot;, &quot;Mary&quot;, &quot;John&quot;) for (i in 1:3) { print(paste(&quot;Hello&quot;, names[i])) } ## [1] &quot;Hello Paul&quot; ## [1] &quot;Hello Mary&quot; ## [1] &quot;Hello John&quot; The counter in a loop does not have to be numeric; it can take on various forms, including strings. By using this flexibility, the same output as generated in the previous code block can be achieved with a different formulation: for (i in c(&quot;Paul&quot;, &quot;Mary&quot;, &quot;John&quot;)) { print(paste(&quot;Hello&quot;, i)) } ## [1] &quot;Hello Paul&quot; ## [1] &quot;Hello Mary&quot; ## [1] &quot;Hello John&quot; 9.8.2 While loops A loop can also be controlled using a while statement, which executes the code until a specified condition is no longer met. This approach is meaningful only if the condition can change based on the operations performed within the loop: cond &lt;- 5 while (cond &gt; 0) { print(cond) cond &lt;- cond - 1 } ## [1] 5 ## [1] 4 ## [1] 3 ## [1] 2 ## [1] 1 Once cond reaches 0, the starting condition is no longer satisfied, and the code will not execute again. It is important to note that while loops can lead to issues if the condition remains true regardless of the code block’s execution. In such cases, the code may become stuck and will need to be manually interrupted. 9.9 apply functions In addition to loops, R offers a more efficient way to perform operations on multiple elements simultaneously. This method, which is often faster, utilizes functions from the apply family: apply, lapply, and sapply. These functions require two key arguments: the list of items to which the operation will be applied and the operation itself. 9.9.1 sapply When the goal is to apply an operation to a vector of elements, the simplest function to use is sapply. It requires two arguments: the vector (X) and the function to be applied (FUN). For illustration, a simple function called func will be created, which adds 1 to an input object: func &lt;- function(x) x + 1 sapply(1:5, func) ## [1] 2 3 4 5 6 The output is a vector of numbers that are each 1 greater than the corresponding elements of the input vector. When this function is applied to a list of numbers, the output becomes a matrix, although the values remain the same: sapply(list(1:5), func) ## [,1] ## [1,] 2 ## [2,] 3 ## [3,] 4 ## [4,] 5 ## [5,] 6 9.9.2 lapply To obtain a list as the output, the lapply function can be used. This function treats the input element X as a list and returns a new list containing the same number of elements as the input. Each element in the output list corresponds to the result of applying FUN to the respective element of the input list: lapply(1:5, func) ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 3 ## ## [[3]] ## [1] 4 ## ## [[4]] ## [1] 5 ## ## [[5]] ## [1] 6 If the input element X is a list, lapply treats the entire list as a single input element, applying FUN to the whole list and returning the result as one element in the output list. An example can help clarify this behavior: lapply(list(1:5), func) ## [[1]] ## [1] 2 3 4 5 6 9.9.3 apply The basic apply function is designed for applying functions to arrays, allowing operations to be performed either on the rows (MARGIN = 1) or on the columns (MARGIN = 2) of the array. While this function may not be used frequently, here are some simple examples of its functionality. For further information, it is advisable to consult the help file or explore online resources, as there are many helpful materials available. mat &lt;- matrix(c(1, 1, 1, 2, 2, 2, 3, 3, 3), c(3, 3)) mat ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 1 2 3 ## [3,] 1 2 3 apply(mat, MARGIN = 1, sum) # adding up all the data in each row ## [1] 6 6 6 apply(mat, MARGIN = 2, sum) # adding up all the data in each column ## [1] 3 6 9 9.10 Exercises on useful R tools Based on the Winters_hours_gaps dataset, use magrittr pipes and functions of the tidyverse to accomplish the following: Convert the dataset into a tibble Select only the top 10 rows of the dataset WHG &lt;- as_tibble(Winters_hours_gaps[1:10, ]) WHG ## # A tibble: 10 × 6 ## Year Month Day Hour Temp_gaps Temp ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2008 3 3 10 15.1 15.1 ## 2 2008 3 3 11 17.2 17.2 ## 3 2008 3 3 12 18.7 18.7 ## 4 2008 3 3 13 18.7 18.7 ## 5 2008 3 3 14 18.8 18.8 ## 6 2008 3 3 15 19.5 19.5 ## 7 2008 3 3 16 19.3 19.3 ## 8 2008 3 3 17 17.7 17.7 ## 9 2008 3 3 18 15.4 15.4 ## 10 2008 3 3 19 12.7 12.7 Convert the tibble to a long format, with separate rows for Temp_gaps and Temp To see the difference between the columns Temp_gaps and Temp, rows 279 to 302 (Julian Day 15) are used below: WHG &lt;- as_tibble(Winters_hours_gaps[279:302, ]) WHGlong &lt;- WHG %&gt;% pivot_longer(cols = Temp_gaps:Temp) WHGlong ## # A tibble: 48 × 6 ## Year Month Day Hour name value ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2008 3 15 0 Temp_gaps 6.76 ## 2 2008 3 15 0 Temp 6.76 ## 3 2008 3 15 1 Temp_gaps 6.48 ## 4 2008 3 15 1 Temp 6.48 ## 5 2008 3 15 2 Temp_gaps 5.51 ## 6 2008 3 15 2 Temp 5.51 ## 7 2008 3 15 3 Temp_gaps 6.89 ## 8 2008 3 15 3 Temp 6.89 ## 9 2008 3 15 4 Temp_gaps 6.10 ## 10 2008 3 15 4 Temp 6.10 ## # ℹ 38 more rows Use ggplot2 to plot Temp_gaps and Temp as facets (point or line plot) ggplot(WHGlong, aes(Hour, value)) + geom_line(lwd = 1.5) + facet_grid(cols = vars(name)) + ylab(&quot;Temperature (°C)&quot;) + theme_bw(base_size = 15) Convert the dataset back to the wide format WHGwide &lt;- WHGlong %&gt;% pivot_wider(names_from = name) WHGwide ## # A tibble: 24 × 6 ## Year Month Day Hour Temp_gaps Temp ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2008 3 15 0 6.76 6.76 ## 2 2008 3 15 1 6.48 6.48 ## 3 2008 3 15 2 5.51 5.51 ## 4 2008 3 15 3 6.89 6.89 ## 5 2008 3 15 4 6.10 6.10 ## 6 2008 3 15 5 NA 6.91 ## 7 2008 3 15 6 NA 6.10 ## 8 2008 3 15 7 NA 5.98 ## 9 2008 3 15 8 NA 8.99 ## 10 2008 3 15 9 10.8 10.8 ## # ℹ 14 more rows Select only the following columns: Year, Month, Day and Temp WHG %&gt;% select(c(Year, Month, Day, Temp)) ## # A tibble: 24 × 4 ## Year Month Day Temp ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2008 3 15 6.76 ## 2 2008 3 15 6.48 ## 3 2008 3 15 5.51 ## 4 2008 3 15 6.89 ## 5 2008 3 15 6.10 ## 6 2008 3 15 6.91 ## 7 2008 3 15 6.10 ## 8 2008 3 15 5.98 ## 9 2008 3 15 8.99 ## 10 2008 3 15 10.8 ## # ℹ 14 more rows Sort the dataset by the Temp column, in descending order WHG %&gt;% arrange(desc(Temp)) ## # A tibble: 24 × 6 ## Year Month Day Hour Temp_gaps Temp ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2008 3 15 13 NA 15.2 ## 2 2008 3 15 16 14.5 14.5 ## 3 2008 3 15 14 NA 14.2 ## 4 2008 3 15 12 NA 14.2 ## 5 2008 3 15 15 14.1 14.1 ## 6 2008 3 15 11 NA 13.6 ## 7 2008 3 15 17 13.3 13.3 ## 8 2008 3 15 18 12.1 12.1 ## 9 2008 3 15 10 12.0 12.0 ## 10 2008 3 15 9 10.8 10.8 ## # ℹ 14 more rows For the Winter_hours_gaps dataset, write a for loop to convert all temperatures (Temp column) to degrees Fahrenheit So that the execution of the following code does not take too long, only Julian Day 15 (rows 279 to 302) is used here. To convert the entire Temp column to Fahrenheit, just omit [279:302] Temp &lt;- Winters_hours_gaps$Temp[279:302] for (i in Temp) { Fahrenheit &lt;- i * 1.8 + 32 print(Fahrenheit) } ## [1] 44.1734 ## [1] 43.6712 ## [1] 41.9252 ## [1] 44.4002 ## [1] 42.9836 ## [1] 44.4452 ## [1] 42.9836 ## [1] 42.755 ## [1] 48.182 ## [1] 51.3698 ## [1] 53.69 ## [1] 56.4692 ## [1] 57.506 ## [1] 59.4446 ## [1] 57.5492 ## [1] 57.3332 ## [1] 58.1522 ## [1] 55.9058 ## [1] 53.8196 ## [1] 49.4708 ## [1] 47.6474 ## [1] 47.3342 ## [1] 48.182 ## [1] 47.7806 Execute the same operation with a function from the apply family Here it is the same as in 2, just omit [279:302] to convert the entire Temp column x &lt;- Winters_hours_gaps$Temp fahrenheit &lt;- function(x) x * 1.8 + 32 sapply(x[279:302], fahrenheit) ## [1] 44.1734 43.6712 41.9252 44.4002 42.9836 44.4452 42.9836 42.7550 48.1820 51.3698 53.6900 ## [12] 56.4692 57.5060 59.4446 57.5492 57.3332 58.1522 55.9058 53.8196 49.4708 47.6474 47.3342 ## [23] 48.1820 47.7806 Now use the tidyverse function mutate to achieve the same outcome WHG_F &lt;- WHG %&gt;% mutate(Temp_F = Temp * 1.8 + 32) WHG_F ## # A tibble: 24 × 7 ## Year Month Day Hour Temp_gaps Temp Temp_F ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2008 3 15 0 6.76 6.76 44.2 ## 2 2008 3 15 1 6.48 6.48 43.7 ## 3 2008 3 15 2 5.51 5.51 41.9 ## 4 2008 3 15 3 6.89 6.89 44.4 ## 5 2008 3 15 4 6.10 6.10 43.0 ## 6 2008 3 15 5 NA 6.91 44.4 ## 7 2008 3 15 6 NA 6.10 43.0 ## 8 2008 3 15 7 NA 5.98 42.8 ## 9 2008 3 15 8 NA 8.99 48.2 ## 10 2008 3 15 9 10.8 10.8 51.4 ## # ℹ 14 more rows "],["getting-temperature-data.html", "Chapter 10 Getting temperature data 10.1 Learning goals for this lesson 10.2 Temperature data needs 10.3 The Global Summary of the Day database 10.4 Exercises on getting temperature data", " Chapter 10 Getting temperature data 10.1 Learning goals for this lesson Appreciate the need for daily temperature data Know how to get a list of promising weather stations contained in an international database Be able to download weather data using chillR functions Know how to convert downloaded data into chillR format 10.2 Temperature data needs Temperature data is essential for phenology and chill modeling, as it serves as a key input for these models. Although weather data might seem easily accessible, it is often challenging to obtain. Many countries have official weather stations that record the necessary information, but access to these data is frequently restricted and expensive, despite likely being funded by taxpayers. While such costs may be manageable for small-scale studies, they quickly become prohibitive for larger analyses. Given the urgent need to understand climate change impacts, these access limitations are a barrier to effective climate research. Such restrictions may also reduce the quality of studies that rely on comprehensive data. Ideally, high-quality, localized datasets would be available, but in their absence, some alternative databases can be used. Currently, chillR connects to one global and one California-specific database, with the potential to expand as more data sources become accessible. 10.3 The Global Summary of the Day database The National Centers for Environmental Information (NCEI), formerly known as the National Climatic Data Center (NCDC), offers valuable temperature data through its Global Summary of the Day (GSOD) database. While accessing this data can be challenging, the bulk download section provides weather records for each station and year. However, finding specific stations can be time-consuming. A list of weather stations is available on NOAA’s website, and the process of downloading and organizing the data can be automated using the chillR tool. A previous user, Adrian Fülle, developed a faster and more efficient method for this task. The chillR function handle_gsod() simplifies the process by guiding users through the necessary steps. 10.3.1 action=list_stations When used with this action, the handle_gsod() function retrieves a list of weather stations and sorts them by their proximity to specified coordinates. For example, stations near Bonn (Latitude: 50.73, Longitude: 7.10) can be identified. Additionally, a time interval, such as 1990-2020, can be set to limit the search to data from those specific years: library(chillR) station_list &lt;- handle_gsod(action = &quot;list_stations&quot;, location = c(7.10, 50.73), time_interval = c(1990, 2020)) require(kableExtra) kable(station_list) %&gt;% kable_styling(&quot;striped&quot;, position = &quot;left&quot;, font_size = 14) %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;500px&quot;) chillR_code STATION.NAME CTRY Lat Long BEGIN END Distance Overlap_years Perc_interval_covered 10517099999 BONN/FRIESDORF(AUT) GM 50.700 7.150 19360102 19921231 4.86 3.00 10 10518099999 BONN-HARDTHOEHE GM 50.700 7.033 19750523 19971223 5.79 7.98 26 10519099999 BONN-ROLEBER GM 50.733 7.200 20010705 20081231 7.07 7.49 24 10513099999 KOLN BONN GM 50.866 7.143 19310101 20230729 15.43 31.00 100 10509099999 BUTZWEILERHOF(BAFB) GM 50.983 6.900 19780901 19950823 31.47 5.64 18 10502099999 NORVENICH GM 50.831 6.658 19730101 20230729 33.14 31.00 100 10514099999 MENDIG GM 50.366 7.315 19730102 19971231 43.26 8.00 26 10506099999 NUERBURG-BARWEILER GM 50.367 6.867 19950401 19971231 43.63 2.75 9 10508099999 BLANKENHEIM GM 50.450 6.650 19781002 19840504 44.56 0.00 0 10510099999 NUERBURG GM 50.333 6.950 19300901 19921231 45.42 3.00 10 10515099999 BENDORF GM 50.417 7.583 19310102 20030816 48.82 13.62 44 10504099999 EIFEL GM 50.650 6.283 20040501 20040501 58.41 0.00 0 10526099999 BAD MARIENBERG GM 50.667 7.967 19730101 20030816 61.65 13.62 44 10613099999 BUCHEL GM 50.174 7.063 19730101 20230729 61.90 31.00 100 10503099999 AACHEN/MERZBRUCK GM 50.817 6.183 19780901 19971212 65.40 7.95 26 10419099999 LUDENSCHEID &amp; GM 51.233 7.600 19270906 20030306 66.06 13.18 43 10400099999 DUSSELDORF GM 51.289 6.767 19310102 20230729 66.43 31.00 100 10616299999 SIEGERLAND GM 50.708 8.083 20040510 20230729 69.46 16.65 54 10418099999 LUEDENSCHEID GM 51.250 7.650 19940301 19971231 69.55 3.84 12 10437499999 MONCHENGLADBACH GM 51.230 6.504 19960715 20230729 69.61 24.47 79 10403099999 MOENCHENGLADBACH GM 51.233 6.500 19381001 19421031 70.05 0.00 0 10501099999 AACHEN GM 50.783 6.100 19280101 20030816 70.81 13.62 44 6496099999 ELSENBORN (MIL) BE 50.467 6.183 19840501 20230729 71.21 31.00 100 10409099999 ESSEN/MUELHEIM GM 51.400 6.967 19300414 19431231 75.12 0.00 0 10410099999 ESSEN/MULHEIM GM 51.400 6.967 19310101 20220408 75.12 31.00 100 The list provided contains the 25 closest weather stations to the specified location, ordered by their distance to the target coordinates, which is shown in the distance column. The Overlap_years column indicates the number of years for which data is available, while the Perc_interval_covered column shows the percentage of the target time period that is covered by the data. It’s important to note that this is based solely on the start and end dates in the table. The dataset may have gaps, and these gaps can sometimes cover almost the entire record. 10.3.2 action=\"download_weather\" When used with this option, the handle_gsod() function downloads the weather data for a specific station, using its unique chillR_code, which is displayed in the respective column of the station list. Instead of manually entering the code, the function can reference the code from the station list. To download the data, the code for the 4th station in the list can be used, as it appears to cover most of the desired time period. weather &lt;- handle_gsod(action = &quot;download_weather&quot;, location = station_list$chillR_code[4], time_interval = c(1990,2020)) The result of this operation is a list with two elements. The first element (weather[[1]]) indicates the source of the database from which the data was retrieved. The second element (weather[[2]]) contains the actual weather dataset, which can be viewed here. weather[[1]][1:20,] X DATE Date Year Month Day Tmin Tmax Tmean Prec YEARMODA Tmin_source Tmax_source no_Tmin no_Tmax 1 1990-01-01 12:00:00 1990-01-01 12:00:00 1990 1 1 -1.000 1.000 0.000 0.000 19900101 original original FALSE FALSE 2 1990-01-02 12:00:00 1990-01-02 12:00:00 1990 1 2 0.000 2.000 1.000 0.000 19900102 original original FALSE FALSE 3 1990-01-03 12:00:00 1990-01-03 12:00:00 1990 1 3 -0.389 2.000 0.722 0.000 19900103 original original FALSE FALSE 4 1990-01-04 12:00:00 1990-01-04 12:00:00 1990 1 4 -1.111 2.000 -0.056 0.000 19900104 original original FALSE FALSE 5 1990-01-05 12:00:00 1990-01-05 12:00:00 1990 1 5 -1.111 3.111 1.556 0.000 19900105 original original FALSE FALSE 6 1990-01-06 12:00:00 1990-01-06 12:00:00 1990 1 6 0.000 2.389 1.333 0.000 19900106 original original FALSE FALSE 7 1990-01-07 12:00:00 1990-01-07 12:00:00 1990 1 7 -0.111 4.278 1.056 0.000 19900107 original original FALSE FALSE 8 1990-01-08 12:00:00 1990-01-08 12:00:00 1990 1 8 -0.111 7.000 3.278 0.000 19900108 original original FALSE FALSE 9 1990-01-09 12:00:00 1990-01-09 12:00:00 1990 1 9 3.778 8.000 5.333 0.508 19900109 original original FALSE FALSE 10 1990-01-10 12:00:00 1990-01-10 12:00:00 1990 1 10 3.000 6.000 4.556 1.016 19900110 original original FALSE FALSE 11 1990-01-11 12:00:00 1990-01-11 12:00:00 1990 1 11 3.278 7.000 5.167 0.254 19900111 original original FALSE FALSE 12 1990-01-12 12:00:00 1990-01-12 12:00:00 1990 1 12 -1.000 5.222 1.778 0.000 19900112 original original FALSE FALSE 13 1990-01-13 12:00:00 1990-01-13 12:00:00 1990 1 13 -1.278 4.000 1.389 0.000 19900113 original original FALSE FALSE 14 1990-01-14 12:00:00 1990-01-14 12:00:00 1990 1 14 -0.222 5.000 3.167 0.000 19900114 original original FALSE FALSE 15 1990-01-15 12:00:00 1990-01-15 12:00:00 1990 1 15 0.889 9.000 4.556 1.016 19900115 original original FALSE FALSE 16 1990-01-16 12:00:00 1990-01-16 12:00:00 1990 1 16 6.222 11.000 9.944 0.000 19900116 original original FALSE FALSE 17 1990-01-17 12:00:00 1990-01-17 12:00:00 1990 1 17 1.000 11.000 8.500 0.000 19900117 original original FALSE FALSE 18 1990-01-18 12:00:00 1990-01-18 12:00:00 1990 1 18 -1.000 7.000 2.722 0.254 19900118 original original FALSE FALSE 19 1990-01-19 12:00:00 1990-01-19 12:00:00 1990 1 19 2.000 7.111 4.611 0.000 19900119 original original FALSE FALSE 20 1990-01-20 12:00:00 1990-01-20 12:00:00 1990 1 20 4.000 8.500 6.056 2.286 19900120 original original FALSE FALSE The data appears complicated and includes unnecessary information. To simplify this, chillR provides a function that streamlines the record. However, this process removes several variables, including quality flags, which may indicate unreliable data. While these have been overlooked so far, there is potential for improvement. 10.3.3 downloaded weather as action argument This method of calling handle_gsod() cleans the dataset and converts it into a format that chillR can easily process. cleaned_weather&lt;-handle_gsod(weather) cleaned_weather[[1]][1:20,] Date Year Month Day Tmin Tmax Tmean Prec 1990-01-01 12:00:00 1990 1 1 -1.000 1.000 0.000 0.000 1990-01-02 12:00:00 1990 1 2 0.000 2.000 1.000 0.000 1990-01-03 12:00:00 1990 1 3 -0.389 2.000 0.722 0.000 1990-01-04 12:00:00 1990 1 4 -1.111 2.000 -0.056 0.000 1990-01-05 12:00:00 1990 1 5 -1.111 3.111 1.556 0.000 1990-01-06 12:00:00 1990 1 6 0.000 2.389 1.333 0.000 1990-01-07 12:00:00 1990 1 7 -0.111 4.278 1.056 0.000 1990-01-08 12:00:00 1990 1 8 -0.111 7.000 3.278 0.000 1990-01-09 12:00:00 1990 1 9 3.778 8.000 5.333 0.508 1990-01-10 12:00:00 1990 1 10 3.000 6.000 4.556 1.016 1990-01-11 12:00:00 1990 1 11 3.278 7.000 5.167 0.254 1990-01-12 12:00:00 1990 1 12 -1.000 5.222 1.778 0.000 1990-01-13 12:00:00 1990 1 13 -1.278 4.000 1.389 0.000 1990-01-14 12:00:00 1990 1 14 -0.222 5.000 3.167 0.000 1990-01-15 12:00:00 1990 1 15 0.889 9.000 4.556 1.016 1990-01-16 12:00:00 1990 1 16 6.222 11.000 9.944 0.000 1990-01-17 12:00:00 1990 1 17 1.000 11.000 8.500 0.000 1990-01-18 12:00:00 1990 1 18 -1.000 7.000 2.722 0.254 1990-01-19 12:00:00 1990 1 19 2.000 7.111 4.611 0.000 1990-01-20 12:00:00 1990 1 20 4.000 8.500 6.056 2.286 The strange numbers in the dataset are due to the original database storing temperatures in degrees Fahrenheit. These values were then converted to degrees Celsius using the formula: \\(Temperature[°C]=(Temperature[°F]-32)\\cdot\\frac{5}{9}\\) This conversion process can result in awkward numbers, but it’s straightforward to apply. After this conversion, the temperature records are now in a more usable format for working with chillR. However, upon closer inspection, the dataset reveals significant gaps, including entire years with missing data. This issue will be addressed in the lesson on filling gaps in temperature records. Additionally, chillR has a similar function for downloading data from the California Irrigation Management Information System (CIMIS), and there is potential for more data sources to be integrated into chillR. Finally, the files generated in this process will be saved for use in upcoming chapters: write.csv(station_list,&quot;data/station_list.csv&quot;, row.names=FALSE) write.csv(weather[[1]],&quot;data/Bonn_raw_weather.csv&quot;, row.names=FALSE) write.csv(cleaned_weather[[1]],&quot;data/Bonn_chillR_weather.csv&quot;, row.names=FALSE) 10.4 Exercises on getting temperature data Choose a location of interest and find the 25 closest weather stations using the handle_gsod function station_list_Yakima &lt;- handle_gsod(action = &quot;list_stations&quot;, location = c(long = -120.50, lat = 46.60), time_interval = c(1990, 2020)) chillR_code STATION.NAME CTRY Lat Long BEGIN END Distance Overlap_years Perc_interval_covered 72781024243 YAKIMA AIR TERMINAL/MCALSR FIELD AP US 46.564 -120.535 19730101 20250303 4.82 31.00 100 99999924243 YAKIMA AIR TERMINAL US 46.568 -120.543 19480101 19721231 4.85 0.00 0 72781399999 VAGABOND AAF / YAKIMA TRAINING CENTER WASHINGTON USA US 46.667 -120.454 20030617 20081110 8.25 5.40 17 72056299999 RANGE OP 13 / YAKIMA TRAINING CENTER US 46.800 -120.167 20080530 20170920 33.79 9.31 30 72788399999 BOWERS FLD US 47.033 -120.531 20000101 20031231 48.26 4.00 13 72788324220 BOWERS FIELD AIRPORT US 47.034 -120.531 19880106 20250303 48.37 31.00 100 99999924220 ELLENSBURG BOWERS FI US 47.034 -120.530 19480601 19550101 48.37 0.00 0 72784094187 HANFORD AIRPORT US 46.567 -119.600 20060101 20130326 68.96 7.23 23 72784099999 HANFORD US 46.567 -119.600 19730101 19971231 68.96 8.00 26 72782594239 PANGBORN MEMORIAL AIRPORT US 47.397 -120.201 20000101 20250303 91.58 21.00 68 72782599999 PANGBORN MEM US 47.399 -120.207 19730101 19971231 91.69 8.00 26 72788499999 RICHLAND AIRPORT US 46.306 -119.304 19810203 20250302 97.39 31.00 100 72781524237 STAMPASS PASS FLTWO US 47.277 -121.337 19730101 20250303 98.63 31.00 100 99999924237 STAMPEDE PASS US 47.277 -121.337 19480101 19721231 98.63 0.00 0 72790024141 EPHRATA MUNICIPAL AIRPORT US 47.308 -119.516 20050101 20250303 108.64 16.00 52 72782624141 EPHRATA MUNICIPAL US 47.308 -119.515 19420101 19971231 108.69 8.00 26 99999924141 EPHRATA AP FCWOS US 47.308 -119.515 19480101 19550101 108.69 0.00 0 72782724110 GRANT COUNTY INTL AIRPORT US 47.193 -119.315 19430610 20250303 111.73 31.00 100 72782799999 MOSES LAKE/GRANT CO US 47.200 -119.317 20000101 20031231 112.06 4.00 13 72784524163 TRI-CITIES AIRPORT US 46.270 -119.118 19730101 20250303 112.21 31.00 100 72784599999 TRI CITIES US 46.267 -119.117 20000101 20031231 112.40 4.00 13 99999924163 PASCO NAS US 46.267 -119.117 19450401 19460601 112.40 0.00 0 72698824219 MUNICIPAL AIRPORT US 45.619 -121.166 19730101 20250303 120.70 31.00 100 99999924219 THE DALLES MUNICIPAL ARPT US 45.619 -121.166 19480101 19650101 120.70 0.00 0 72688399999 HERMISTON MUNI US 45.828 -119.259 19980514 20051231 128.55 7.64 25 Download weather data for the most promising station on the list weather_Yakima &lt;- handle_gsod(action = &quot;download_weather&quot;, location = station_list_Yakima$chillR_code[1], time_interval = c(1990, 2020)) ## Loading data for 31 years from station &#39;YAKIMA AIR TERMINAL/MCALSR FIELD AP&#39; ## ============================================================================================== weather_Yakima[[1]][1:20,] STATION DATE LATITUDE LONGITUDE ELEVATION NAME TEMP TEMP_ATTRIBUTES DEWP DEWP_ATTRIBUTES SLP SLP_ATTRIBUTES STP STP_ATTRIBUTES VISIB VISIB_ATTRIBUTES WDSP WDSP_ATTRIBUTES MXSPD GUST MAX MAX_ATTRIBUTES MIN MIN_ATTRIBUTES PRCP PRCP_ATTRIBUTES SNDP FRSHTT 72781024243 1990-01-01 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 31.1 24 30.2 24 1014.9 24 975.3 24 2.3 24 4.1 24 7.0 999.9 35.1 28.9 0.00 D 999.9 100000 72781024243 1990-01-02 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 32.1 24 23.7 24 1015.4 24 975.7 24 24.3 24 6.2 24 12.0 18.1 43.0 21.9 0.00 D 999.9 100000 72781024243 1990-01-03 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 32.1 24 24.9 24 1023.5 24 983.6 24 21.3 24 3.7 24 8.0 999.9 46.0 21.9 0.00 D 999.9 0 72781024243 1990-01-04 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 36.4 24 29.1 24 1020.2 24 980.5 24 21.3 24 6.1 24 15.9 24.1 57.0 26.1 0.00 D 999.9 0 72781024243 1990-01-05 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 36.6 24 27.5 24 1022.7 24 983.1 24 21.3 24 5.8 24 18.1 25.1 57.0 26.1 0.00 D 999.9 0 72781024243 1990-01-06 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 43.2 24 34.6 24 1016.9 24 977.7 24 25.1 24 6.8 24 22.9 28.0 53.1 26.1 0.00 D 999.9 10000 72781024243 1990-01-07 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 48.2 24 39.5 24 1005.8 24 967.2 24 27.5 24 9.6 24 15.9 26.0 53.1 28.0 0.12 G 999.9 10000 72781024243 1990-01-08 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 44.1 24 37.4 24 1005.4 24 966.8 24 27.5 24 7.6 24 18.1 27.0 54.0 37.0 0.38 G 999.9 10000 72781024243 1990-01-09 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 39.4 24 36.9 24 1010.6 24 971.6 24 10.8 24 5.7 24 15.0 999.9 52.0 34.0 0.62 G 999.9 110000 72781024243 1990-01-10 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 44.9 24 29.3 24 1019.1 24 980.0 24 29.6 24 8.5 24 18.1 22.0 54.0 34.0 0.11 G 999.9 10000 72781024243 1990-01-11 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 37.0 24 26.9 24 1030.3 24 990.4 24 25.3 24 6.8 24 11.1 999.9 46.9 32.0 0.00 G 999.9 1000 72781024243 1990-01-12 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 37.1 24 27.0 24 1020.2 24 980.7 24 25.3 24 6.4 24 10.1 999.9 43.0 32.0 0.01 G 999.9 11000 72781024243 1990-01-13 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 33.7 24 29.3 24 1010.4 24 971.2 24 17.0 24 3.0 24 8.0 999.9 43.0 27.0 0.01 G 999.9 10000 72781024243 1990-01-14 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 31.8 24 28.4 24 1015.3 24 975.7 24 16.4 24 3.9 24 8.0 999.9 41.0 25.0 0.00 D 999.9 0 72781024243 1990-01-15 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 33.6 24 30.9 24 1021.3 24 981.5 24 2.5 24 3.1 24 7.0 999.9 39.9 25.0 0.00 D 999.9 100000 72781024243 1990-01-16 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 38.4 24 33.4 24 1016.9 24 977.5 24 11.1 24 5.2 24 8.9 999.9 45.0 30.9 0.00 D 999.9 110000 72781024243 1990-01-17 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 34.0 24 25.7 24 1023.9 24 984.1 24 30.2 24 6.3 24 10.1 999.9 46.0 24.1 0.00 D 999.9 0 72781024243 1990-01-18 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 27.2 24 21.6 24 1028.8 24 988.5 24 26.1 24 5.2 24 8.9 999.9 46.9 18.0 0.00 D 999.9 0 72781024243 1990-01-19 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 32.7 24 27.4 24 1024.2 24 984.2 24 21.7 24 5.4 24 8.9 999.9 37.9 18.0 0.00 D 999.9 0 72781024243 1990-01-20 46.56398 -120.5349 321 YAKIMA AIRPORT, WA US 33.4 24 26.4 24 1027.7 24 987.7 24 14.3 24 1.3 24 6.0 999.9 36.0 28.0 0.00 D 999.9 0 Convert the weather data into chillR format cleaned_weather_Yakima &lt;- handle_gsod(weather_Yakima) cleaned_weather_Yakima[[1]][1:20,] Date Year Month Day Tmin Tmax Tmean Prec 1990-01-01 1990 1 1 -1.722 1.722 -0.500 0.000 1990-01-02 1990 1 2 -5.611 6.111 0.056 0.000 1990-01-03 1990 1 3 -5.611 7.778 0.056 0.000 1990-01-04 1990 1 4 -3.278 13.889 2.444 0.000 1990-01-05 1990 1 5 -3.278 13.889 2.556 0.000 1990-01-06 1990 1 6 -3.278 11.722 6.222 0.000 1990-01-07 1990 1 7 -2.222 11.722 9.000 3.048 1990-01-08 1990 1 8 2.778 12.222 6.722 9.652 1990-01-09 1990 1 9 1.111 11.111 4.111 15.748 1990-01-10 1990 1 10 1.111 12.222 7.167 2.794 1990-01-11 1990 1 11 0.000 8.278 2.778 0.000 1990-01-12 1990 1 12 0.000 6.111 2.833 0.254 1990-01-13 1990 1 13 -2.778 6.111 0.944 0.254 1990-01-14 1990 1 14 -3.889 5.000 -0.111 0.000 1990-01-15 1990 1 15 -3.889 4.389 0.889 0.000 1990-01-16 1990 1 16 -0.611 7.222 3.556 0.000 1990-01-17 1990 1 17 -4.389 7.778 1.111 0.000 1990-01-18 1990 1 18 -7.778 8.278 -2.667 0.000 1990-01-19 1990 1 19 -7.778 3.278 0.389 0.000 1990-01-20 1990 1 20 -2.222 2.222 0.778 0.000 dir.create(&quot;Yakima&quot;) write.csv(station_list_Yakima,&quot;Yakima/station_list.csv&quot;, row.names = FALSE) write.csv(weather_Yakima[[1]],&quot;Yakima/raw_weather.csv&quot;, row.names = FALSE) write.csv(cleaned_weather_Yakima[[1]],&quot;Yakima/chillR_weather.csv&quot;, row.names = FALSE) "],["filling-gaps-in-temperature-records.html", "Chapter 11 Filling gaps in temperature records 11.1 Learning goals 11.2 Gaps 11.3 Filling short gaps in daily records 11.4 Filling long gaps in daily records 11.5 Exercises on filling gaps", " Chapter 11 Filling gaps in temperature records 11.1 Learning goals see why having gaps in records can be quite problematic learn about simple ways to fill gaps in daily temperature records learn how to use data from auxiliary weather stations to fill gaps in daily temperature records learn about a creative way to close gaps in hourly temperature records 11.2 Gaps There is a wealth of weather data, but it often contains gaps due to issues like equipment malfunctions, power outages, and storage problems. These gaps pose challenges for modeling agroclimatic conditions, as many scientific methods struggle with missing data. Therefore, effective gap-filling methods are necessary. 11.3 Filling short gaps in daily records Weather records are mostly complete, though some daily maximum or minimum temperatures may be missing. For short gaps, simple linear interpolation can estimate values by averaging the last known and first known measurements around the gap. This approach can work for gaps of 2-3 days, though accuracy declines with longer gaps. For extended absences of several weeks or more, linear interpolation may miss significant temperature trends, and for gaps as long as a year, it could entirely overlook seasonal patterns, leading to large errors. Nonetheless, the chillR package provides a function, interpolate_gaps(), to perform this basic interpolation: weather &lt;- KA_weather %&gt;% make_all_day_table() Tmin_int &lt;- interpolate_gaps(weather[,&quot;Tmin&quot;]) weather &lt;- weather %&gt;% mutate(Tmin = Tmin_int$interp, Tmin_interpolated = Tmin_int$missing) Tmax_int &lt;- interpolate_gaps(weather[,&quot;Tmax&quot;]) weather &lt;- weather %&gt;% mutate(Tmax = Tmax_int$interp, Tmax_interpolated = Tmax_int$missing) The fix_weather() function in chillR uses linear interpolation to fill gaps in minimum and maximum temperature data by default. If any entire days are missing, it adds rows for these dates using make_all_day_table(). Users can also customize the function by setting the range of years (with start_year and end_year), specifying specific dates (using start_date and end_date in Julian days), and choosing column names if they differ from Tmin and Tmax. # add an extra day to the KA_weather dataset that is not connected to the days that are already there. # this creates a large gap, which we can then interpolate KA_weather_gap &lt;- rbind(KA_weather, c(Year = 2011, Month = 3, Day = 3, Tmax = 26, Tmin = 14)) # fill in the gaps between Julian date 300 (late October) and 100 (early April), only returning data between 2000 and 2011 fixed_winter_days &lt;- KA_weather_gap %&gt;% fix_weather(start_year = 2000, end_year = 2011, start_date = 300, end_date = 100) # fill in all gaps fixed_all_days &lt;- KA_weather_gap %&gt;% fix_weather() The fix_weather() function returns a list with two components: weather: A data.frame containing the interpolated weather data. It includes two additional columns, no_Tmin and no_Tmax, which mark rows where minimum or maximum temperatures were originally missing (indicated as TRUE, otherwise FALSE). QC: A quality control object that summarizes the number of interpolated values for each season. These QC elements provide an overview of the data quality and interpolation extent for each dataset processed: fixed_winter_days$QC Season End_year Season_days Data_days Missing_Tmin Missing_Tmax Incomplete_days Perc_complete 1999/2000 2000 166 100 66 66 66 60.2 2000/2001 2001 167 167 0 0 0 100.0 2001/2002 2002 166 166 0 0 0 100.0 2002/2003 2003 166 166 0 0 0 100.0 2003/2004 2004 166 166 0 0 0 100.0 2004/2005 2005 167 167 0 0 0 100.0 2005/2006 2006 166 166 0 0 0 100.0 2006/2007 2007 166 166 0 0 0 100.0 2007/2008 2008 166 166 0 0 0 100.0 2008/2009 2009 167 167 0 0 0 100.0 2009/2010 2010 166 166 0 0 0 100.0 2010/2011 2011 166 128 165 165 165 0.6 fixed_all_days$QC Season End_year Season_days Data_days Missing_Tmin Missing_Tmax Incomplete_days Perc_complete 1997/1998 1998 365 365 0 0 0 100.0 1998/1999 1999 365 365 0 0 0 100.0 1999/2000 2000 366 366 0 0 0 100.0 2000/2001 2001 365 365 0 0 0 100.0 2001/2002 2002 365 365 0 0 0 100.0 2002/2003 2003 365 365 0 0 0 100.0 2003/2004 2004 366 366 0 0 0 100.0 2004/2005 2005 365 365 0 0 0 100.0 2005/2006 2006 365 365 0 0 0 100.0 2006/2007 2007 365 365 0 0 0 100.0 2007/2008 2008 366 366 0 0 0 100.0 2008/2009 2009 365 365 0 0 0 100.0 2009/2010 2010 365 365 214 214 214 41.4 2010/2011 2011 365 62 364 364 364 0.3 As mentioned, linear interpolation works reasonably well for short gaps in data but becomes less reliable as the gaps lengthen. Below is a simple demonstration of this effect: gap_weather &lt;- KA_weather[200:305, ] gap_weather[ ,&quot;Tmin_observed&quot;] &lt;- gap_weather$Tmin gap_weather$Tmin[c(2, 4:5, 7:9, 11:14, 16:20, 22:27, 29:35, 37:44, 46:54, 56:65, 67:77, 79:90, 92:104)] &lt;- NA fixed_gaps &lt;- fix_weather(gap_weather)$weather ggplot(data = fixed_gaps, aes(DATE, Tmin_observed)) + geom_line(lwd = 1.3) + xlab(&quot;Date&quot;) + ylab(&quot;Daily minimum temperature (°C)&quot;) + geom_line(data = fixed_gaps, aes(DATE, Tmin), col = &quot;red&quot;, lwd = 1.3) The plot illustrates original temperature measurements (in black) alongside interpolated values (in red). To simulate gaps, values were removed incrementally from the dataset - starting with single missing values on the left and increasing up to 13 consecutive missing values on the right. The results show that for shorter gaps (left side), interpolation closely follows the temperature trends. However, as gaps grow longer (right side), the interpolated values deviate more significantly, failing to capture the actual temperature dynamics accurately. This visual demonstrates the diminishing reliability of linear interpolation as gaps increase in size. fixed_gaps[,&quot;error&quot;] &lt;- abs(fixed_gaps$Tmin - fixed_gaps$Tmin_observed) ggplot(data = fixed_gaps, aes(DATE, error)) + geom_line(lwd = 1.3) + xlab(&quot;Date&quot;) + ylab(&quot;Error introduced by interpolation (°C)&quot;) + geom_point(data = fixed_gaps[which(!fixed_gaps$no_Tmin),], aes(DATE, error),col = &quot;red&quot;, cex = 3) The size of interpolation errors depends on temperature variation during the missing periods - high variation leads to larger errors. As gap sizes increase, errors tend to grow, especially in the middle of large gaps, where interpolated values can be far from actual conditions. While errors are zero at points where data exists, larger gaps clearly require a more advanced method for accurate estimation. 11.4 Filling long gaps in daily records Long gaps in temperature records are a persistent issue because actual temperatures at specific times and locations are unknown if no measurements were taken. While linear interpolation can provide reasonable estimates for short gaps, longer gaps require more reliable approaches. Instead of using complex interpolation algorithms, an effective solution can be to incorporate additional data from nearby weather stations in climatically similar settings. Typically, if another station close to the target site has a similar climate, its data can be used to fill in gaps. For example, this method is used in the CIMIS network in California, where data from nearby stations is often used to address missing values. However, this approach has limitations, as even small differences in elevation or unique landscape features (like lakes, forests, or coastlines) can lead to climate variations that make nearby stations less accurate proxies. In chillR, the patch_weather() function addresses these limitations by using data from auxiliary stations to fill gaps. It can detect certain biases, adjust for mean temperature biases, and apply the corrected data to the location of interest, thus providing a more refined way to handle missing temperature records. To fill gaps in the temperature dataset for Bonn, downloaded in the Getting temperature data lesson, the following steps should be taken: Bonn &lt;- read.csv(&quot;data/Bonn_chillR_weather.csv&quot;) Bonn_QC &lt;- fix_weather(Bonn)$QC Bonn_QC Season End_year Season_days Data_days Missing_Tmin Missing_Tmax Incomplete_days Perc_complete 1989/1990 1990 365 365 0 0 0 100.0 1990/1991 1991 365 365 0 0 0 100.0 1991/1992 1992 366 366 0 0 0 100.0 1992/1993 1993 365 365 0 0 0 100.0 1993/1994 1994 365 365 0 0 0 100.0 1994/1995 1995 365 365 0 0 0 100.0 1995/1996 1996 366 366 0 0 0 100.0 1996/1997 1997 365 365 0 0 0 100.0 1997/1998 1998 365 365 4 4 4 98.9 1998/1999 1999 365 365 365 365 365 0.0 1999/2000 2000 366 366 366 366 366 0.0 2000/2001 2001 365 365 365 365 365 0.0 2001/2002 2002 365 365 365 365 365 0.0 2002/2003 2003 365 365 316 316 316 13.4 2003/2004 2004 366 366 366 366 366 0.0 2004/2005 2005 365 365 0 0 0 100.0 2005/2006 2006 365 365 0 0 0 100.0 2006/2007 2007 365 365 0 0 0 100.0 2007/2008 2008 366 366 0 0 0 100.0 2008/2009 2009 365 365 0 0 0 100.0 2009/2010 2010 365 365 0 0 0 100.0 2010/2011 2011 365 365 0 0 0 100.0 2011/2012 2012 366 366 0 0 0 100.0 2012/2013 2013 365 365 0 0 0 100.0 2013/2014 2014 365 365 0 0 0 100.0 2014/2015 2015 365 365 0 0 0 100.0 2015/2016 2016 366 366 0 0 0 100.0 2016/2017 2017 365 365 0 0 0 100.0 2017/2018 2018 365 365 0 0 0 100.0 2018/2019 2019 365 365 0 0 0 100.0 2019/2020 2020 366 366 0 0 0 100.0 The dataset for Bonn has significant gaps between 1998 and 2004, as well as in 2008 (where almost all values are missing), along with shorter gaps in 2015, 2018, and 2020. To address these gaps, data from nearby weather stations is required. The handle_gsod() function can be used to find relevant stations in the surrounding area. station_list &lt;- handle_gsod(action = &quot;list_stations&quot;, location = c(7.10, 50.73), time_interval = c(1990, 2020)) ## chillR_code STATION.NAME CTRY Lat Long BEGIN END Distance Overlap_years ## 1 10517099999 BONN/FRIESDORF(AUT) GM 50.700 7.150 19360102 19921231 4.86 3.00 ## 2 10518099999 BONN-HARDTHOEHE GM 50.700 7.033 19750523 19971223 5.79 7.98 ## 3 10519099999 BONN-ROLEBER GM 50.733 7.200 20010705 20081231 7.07 7.49 ## 4 10513099999 KOLN BONN GM 50.866 7.143 19310101 20230729 15.43 31.00 ## 5 10509099999 BUTZWEILERHOF(BAFB) GM 50.983 6.900 19780901 19950823 31.47 5.64 ## 6 10502099999 NORVENICH GM 50.831 6.658 19730101 20230729 33.14 31.00 ## 7 10514099999 MENDIG GM 50.366 7.315 19730102 19971231 43.26 8.00 ## 8 10506099999 NUERBURG-BARWEILER GM 50.367 6.867 19950401 19971231 43.63 2.75 ## 9 10508099999 BLANKENHEIM GM 50.450 6.650 19781002 19840504 44.56 0.00 ## 10 10510099999 NUERBURG GM 50.333 6.950 19300901 19921231 45.42 3.00 ## 11 10515099999 BENDORF GM 50.417 7.583 19310102 20030816 48.82 13.62 ## 12 10504099999 EIFEL GM 50.650 6.283 20040501 20040501 58.41 0.00 ## 13 10526099999 BAD MARIENBERG GM 50.667 7.967 19730101 20030816 61.65 13.62 ## 14 10613099999 BUCHEL GM 50.174 7.063 19730101 20230729 61.90 31.00 ## 15 10503099999 AACHEN/MERZBRUCK GM 50.817 6.183 19780901 19971212 65.40 7.95 ## 16 10419099999 LUDENSCHEID &amp; GM 51.233 7.600 19270906 20030306 66.06 13.18 ## 17 10400099999 DUSSELDORF GM 51.289 6.767 19310102 20230729 66.43 31.00 ## 18 10616299999 SIEGERLAND GM 50.708 8.083 20040510 20230729 69.46 16.65 ## 19 10418099999 LUEDENSCHEID GM 51.250 7.650 19940301 19971231 69.55 3.84 ## 20 10437499999 MONCHENGLADBACH GM 51.230 6.504 19960715 20230729 69.61 24.47 ## 21 10403099999 MOENCHENGLADBACH GM 51.233 6.500 19381001 19421031 70.05 0.00 ## 22 10501099999 AACHEN GM 50.783 6.100 19280101 20030816 70.81 13.62 ## 23 6496099999 ELSENBORN (MIL) BE 50.467 6.183 19840501 20230729 71.21 31.00 ## 24 10409099999 ESSEN/MUELHEIM GM 51.400 6.967 19300414 19431231 75.12 0.00 ## 25 10410099999 ESSEN/MULHEIM GM 51.400 6.967 19310101 20220408 75.12 31.00 ## Perc_interval_covered ## 1 10 ## 2 26 ## 3 24 ## 4 100 ## 5 18 ## 6 100 ## 7 26 ## 8 9 ## 9 0 ## 10 10 ## 11 44 ## 12 0 ## 13 44 ## 14 100 ## 15 26 ## 16 43 ## 17 100 ## 18 54 ## 19 12 ## 20 79 ## 21 0 ## 22 44 ## 23 100 ## 24 0 ## 25 100 chillR_code STATION.NAME CTRY Lat Long BEGIN END Distance Overlap_years Perc_interval_covered 10517099999 BONN/FRIESDORF(AUT) GM 50.700 7.150 19360102 19921231 4.86 3.00 10 10518099999 BONN-HARDTHOEHE GM 50.700 7.033 19750523 19971223 5.79 7.98 26 10519099999 BONN-ROLEBER GM 50.733 7.200 20010705 20081231 7.07 7.49 24 10513099999 KOLN BONN GM 50.866 7.143 19310101 20230729 15.43 31.00 100 10509099999 BUTZWEILERHOF(BAFB) GM 50.983 6.900 19780901 19950823 31.47 5.64 18 10502099999 NORVENICH GM 50.831 6.658 19730101 20230729 33.14 31.00 100 10514099999 MENDIG GM 50.366 7.315 19730102 19971231 43.26 8.00 26 10506099999 NUERBURG-BARWEILER GM 50.367 6.867 19950401 19971231 43.63 2.75 9 10508099999 BLANKENHEIM GM 50.450 6.650 19781002 19840504 44.56 0.00 0 10510099999 NUERBURG GM 50.333 6.950 19300901 19921231 45.42 3.00 10 10515099999 BENDORF GM 50.417 7.583 19310102 20030816 48.82 13.62 44 10504099999 EIFEL GM 50.650 6.283 20040501 20040501 58.41 0.00 0 10526099999 BAD MARIENBERG GM 50.667 7.967 19730101 20030816 61.65 13.62 44 10613099999 BUCHEL GM 50.174 7.063 19730101 20230729 61.90 31.00 100 10503099999 AACHEN/MERZBRUCK GM 50.817 6.183 19780901 19971212 65.40 7.95 26 10419099999 LUDENSCHEID &amp; GM 51.233 7.600 19270906 20030306 66.06 13.18 43 10400099999 DUSSELDORF GM 51.289 6.767 19310102 20230729 66.43 31.00 100 10616299999 SIEGERLAND GM 50.708 8.083 20040510 20230729 69.46 16.65 54 10418099999 LUEDENSCHEID GM 51.250 7.650 19940301 19971231 69.55 3.84 12 10437499999 MONCHENGLADBACH GM 51.230 6.504 19960715 20230729 69.61 24.47 79 10403099999 MOENCHENGLADBACH GM 51.233 6.500 19381001 19421031 70.05 0.00 0 10501099999 AACHEN GM 50.783 6.100 19280101 20030816 70.81 13.62 44 6496099999 ELSENBORN (MIL) BE 50.467 6.183 19840501 20230729 71.21 31.00 100 10409099999 ESSEN/MUELHEIM GM 51.400 6.967 19300414 19431231 75.12 0.00 0 10410099999 ESSEN/MULHEIM GM 51.400 6.967 19310101 20220408 75.12 31.00 100 Many of the stations listed may not be useful, as they only overlap with the existing record for a few years, or not at all. As a result, it’s unlikely that any single station will be able to fill all the gaps in Bonn’s temperature data. However, by combining data from multiple auxiliary stations, it may be possible to fill in the missing values. Promising stations include BONN-HARDTHOEHE, BONN-ROLEBER, and NORVENICH. These stations will be downloaded and stored in a list. In chillR version 0.74 and later, the handle_gsod() function can download multiple station records at once, returning them as a named list. This function will be used to download the records for the selected stations (positions 2, 3, and 6 in the station list). patch_weather &lt;- handle_gsod(action = &quot;download_weather&quot;, location = as.character(station_list$chillR_code[c(2, 3, 6)]), time_interval = c(1990, 2020)) %&gt;% handle_gsod() ## Loading data for 31 years from station &#39;NORVENICH&#39; ## ============================================================================================== ## ## Loading data for 31 years from station &#39;BONN-HARDTHOEHE&#39; ## ============================================================================================== ## ## Loading data for 31 years from station &#39;BONN-ROLEBER&#39; ## ============================================================================================== With the list of potentially useful weather records now available, the next step is to use the patch_daily_temperatures() function. This function will integrate the data from the selected auxiliary stations to fill the missing temperature values in the Bonn dataset: patched &lt;- patch_daily_temperatures(weather = Bonn, patch_weather = patch_weather) The results can be reviewed by examining the statistics element of the patched object, accessed using patched$statistics. This will provide an overview of how the gaps were filled and the adjustments made using the auxiliary station data. # Patch statistics for NORVENICH patched$statistics[[1]] mean_bias stdev_bias filled gaps_remain Tmin -0.307 1.304 2146 1 Tmax 0.202 1.154 2146 1 # Patch statistics for BONN-HARDTHOEHE patched$statistics[[2]] mean_bias stdev_bias filled gaps_remain Tmin -1.871 2.080 0 1 Tmax 1.466 1.427 0 1 # Patch statistics for BONN-ROLEBER patched$statistics[[3]] mean_bias stdev_bias filled gaps_remain Tmin -0.546 1.186 0 1 Tmax 1.314 1.089 0 1 The analysis provides insight into the similarity between the temperature records from the auxiliary stations and the target station in Bonn, specifically for both Tmin and Tmax. It includes the following statistics: filled: The number of days for which data was successfully transferred from each auxiliary station. gaps_remain: The number of gaps still present after the patching process. Additionally, two quality statistics are provided: Mean Bias (mean_bias): The average temperature difference between the auxiliary station and the Bonn station. This can be easily addressed by adjusting daily temperature values when transferring data between stations, and the patch_daily_temperatures() function automatically corrects for this. Standard Deviation of Daily Differences (stdev_bias): This indicates the variability in the differences between the stations. A high stdev_bias suggests that the temperature differences are not systematic and may not be easily corrected. If the stdev_bias exceeds a certain threshold, it may be best to exclude that station from the data transfer. To address these metrics, limits can be set for both. The mean_bias can be capped at 1°C, and the stdev_bias can be capped at 2°C. These thresholds will be passed as parameters (max_mean_bias and max_stdev_bias) to the patch_daily_temperatures() function. After setting these limits, the statistics should be reviewed again to ensure the quality of the patching process. patched &lt;- patch_daily_temperatures(weather = Bonn, patch_weather = patch_weather, max_mean_bias = 1, max_stdev_bias = 2) # Patch statistics for NORVENICH patched$statistics[[1]] mean_bias stdev_bias filled gaps_remain Tmin -0.307 1.304 2146 1 Tmax 0.202 1.154 2146 1 # Patch statistics for BONN-HARDTHOEHE patched$statistics[[2]] mean_bias stdev_bias filled gaps_remain Tmin -1.871 2.080 0 1 Tmax 1.466 1.427 0 1 # Patch statistics for BONN-ROLEBER patched$statistics[[3]] mean_bias stdev_bias filled gaps_remain Tmin -0.546 1.186 0 1 Tmax 1.314 1.089 0 1 After applying the filters, all records from BONN-HARDTHOEHE and the Tmax records from BONN-ROLEBER were rejected because they did not meet the mean_bias threshold. However, the data from NORVENICH were deemed suitable, allowing us to fill 2,146 gaps for both Tmin and Tmax, leaving just 1 gap remaining for each. To identify where the remaining gaps are, the next step is to use the fix_weather() function. This will allow us to examine the dataset and pinpoint the exact locations of the remaining gaps in both Tmin and Tmax. post_patch_stats &lt;- fix_weather(patched)$QC post_patch_stats Season End_year Season_days Data_days Missing_Tmin Missing_Tmax Incomplete_days Perc_complete 1989/1990 1990 365 365 0 0 0 100.0 1990/1991 1991 365 365 0 0 0 100.0 1991/1992 1992 366 366 0 0 0 100.0 1992/1993 1993 365 365 0 0 0 100.0 1993/1994 1994 365 365 0 0 0 100.0 1994/1995 1995 365 365 0 0 0 100.0 1995/1996 1996 366 366 0 0 0 100.0 1996/1997 1997 365 365 0 0 0 100.0 1997/1998 1998 365 365 0 0 0 100.0 1998/1999 1999 365 365 1 1 1 99.7 1999/2000 2000 366 366 0 0 0 100.0 2000/2001 2001 365 365 0 0 0 100.0 2001/2002 2002 365 365 0 0 0 100.0 2002/2003 2003 365 365 0 0 0 100.0 2003/2004 2004 366 366 0 0 0 100.0 2004/2005 2005 365 365 0 0 0 100.0 2005/2006 2006 365 365 0 0 0 100.0 2006/2007 2007 365 365 0 0 0 100.0 2007/2008 2008 366 366 0 0 0 100.0 2008/2009 2009 365 365 0 0 0 100.0 2009/2010 2010 365 365 0 0 0 100.0 2010/2011 2011 365 365 0 0 0 100.0 2011/2012 2012 366 366 0 0 0 100.0 2012/2013 2013 365 365 0 0 0 100.0 2013/2014 2014 365 365 0 0 0 100.0 2014/2015 2015 365 365 0 0 0 100.0 2015/2016 2016 366 366 0 0 0 100.0 2016/2017 2017 365 365 0 0 0 100.0 2017/2018 2018 365 365 0 0 0 100.0 2018/2019 2019 365 365 0 0 0 100.0 2019/2020 2020 366 366 0 0 0 100.0 After patching the dataset, only a single day’s data remains missing, which is a very short gap. Given its small size, it is reasonable to use linear interpolation to fill this remaining gap. The fix_weather() function can be used to interpolate this final missing value and complete the dataset. Bonn_weather &lt;- fix_weather(patched) 11.4.1 Bias-correction for shorter intervals In the patch_daily_temperatures() function, the bias correction is typically based on the average temperature difference between two weather stations over the entire year. However, this approach assumes that the bias between stations is constant throughout the year, which may not be true. In reality, the bias can vary across different seasons, meaning a station might be a good proxy for temperature data during certain times of the year but not others. To address this, the patch_daily_temps() function (note the slight name difference) allows for seasonally adjusted bias correction. By default, this function evaluates temperature data on a monthly basis, making separate comparisons for each calendar month. It can then assess whether an auxiliary station is a reliable proxy for temperatures in each specific month and apply month-specific bias corrections, leading to more accurate estimations with smaller biases than if the entire year were used for comparison. patched_monthly &lt;- patch_daily_temps(weather = Bonn, patch_weather = patch_weather, max_mean_bias = 1, max_stdev_bias = 2, time_interval = &quot;month&quot;) The findings for minimum temperatures from the NORVENICH station can be summarized as follows: patched_monthly$statistics$Tmin$NORVENICH Interval Total_days Overlap_days Mean_bias Stdev_bias Gaps_before Filled Gaps_remain 1 961 773 0.184 1.260 186 185 1 2 961 774 0.281 1.242 186 186 0 3 876 706 0.269 1.248 170 170 0 4 961 775 0.253 1.427 186 186 0 5 930 758 0.509 1.431 166 166 0 6 961 801 0.373 1.238 158 158 0 7 930 749 0.396 1.210 180 180 0 8 961 781 0.480 1.305 179 179 0 9 961 774 0.529 1.302 186 186 0 10 930 750 0.205 1.264 180 180 0 11 961 775 0.098 1.307 186 186 0 12 930 745 0.101 1.308 184 184 0 The analysis shows that the mean bias between the stations varies significantly throughout the year, indicating that a month-specific bias correction would likely improve the accuracy of the temperature data. To implement this, the time_interval parameter in the patch_daily_temps() function allows for flexibility in the interval used for bias correction. Intervals can be set to month, week, or even custom durations like 10 days or 2 weeks. The function will start counting these intervals from January 1st of each year, which might result in shorter intervals toward the end of the year. This can generate warnings if the final interval is smaller than the selected period. However, it’s important to keep in mind that using smaller intervals reduces the amount of data available for bias determination. For short time series, very short intervals may not provide enough data, which could reduce the reliability of the bias correction. Therefore, selecting an appropriate interval size is crucial to balancing accuracy and data availability. patched_2weeks &lt;- patch_daily_temps(weather = Bonn, patch_weather = patch_weather, max_mean_bias = 1, max_stdev_bias = 2, time_interval = &quot;2 weeks&quot;) To demonstrate the effects, 5000 gaps will be created in the Bonn weather record and filled with proxy data using annual, monthly, and bi-weekly intervals for bias evaluation. The resulting errors will be visualized with a violin plot using ggplot2. Gaps &lt;- sample(seq(1:nrow(Bonn)), size = 5000, replace = FALSE) Bonn_gaps &lt;- Bonn %&gt;% mutate(obs_Tmin=Tmin, obs_Tmax=Tmax) Bonn_gaps$Tmin[Gaps] &lt;- NA Bonn_gaps$Tmax[Gaps] &lt;- NA patch_annual &lt;- patch_daily_temps(weather = Bonn_gaps, patch_weather = patch_weather, max_mean_bias = 1, max_stdev_bias = 2, time_interval = &quot;year&quot;) patch_month &lt;- patch_daily_temps(weather = Bonn_gaps, patch_weather = patch_weather, max_mean_bias = 1, max_stdev_bias = 2, time_interval = &quot;month&quot;) patch_2weeks &lt;- patch_daily_temps(weather = Bonn_gaps, patch_weather = patch_weather, max_mean_bias = 1, max_stdev_bias = 2, time_interval = &quot;2 weeks&quot;) Bonn_gaps[,&quot;Tmin_annual&quot;] &lt;- Bonn_gaps$obs_Tmin - patch_annual$weather$Tmin Bonn_gaps[,&quot;Tmax_annual&quot;] &lt;- Bonn_gaps$obs_Tmax - patch_annual$weather$Tmax Bonn_gaps[,&quot;Tmin_month&quot;] &lt;- Bonn_gaps$obs_Tmin - patch_month$weather$Tmin Bonn_gaps[,&quot;Tmax_month&quot;] &lt;- Bonn_gaps$obs_Tmax - patch_month$weather$Tmax Bonn_gaps[,&quot;Tmin_2weeks&quot;] &lt;- Bonn_gaps$obs_Tmin - patch_2weeks$weather$Tmin Bonn_gaps[,&quot;Tmax_2weeks&quot;] &lt;- Bonn_gaps$obs_Tmax - patch_2weeks$weather$Tmax Interval_eval &lt;- Bonn_gaps %&gt;% filter(is.na(Tmin)) %&gt;% pivot_longer(Tmin_annual:Tmax_2weeks) %&gt;% mutate(Type=factor(name, levels = c(&quot;Tmin_annual&quot;, &quot;Tmin_month&quot;, &quot;Tmin_2weeks&quot;, &quot;Tmax_annual&quot;, &quot;Tmax_month&quot;, &quot;Tmax_2weeks&quot;)) ) ggplot(Interval_eval, aes(Type,value)) + geom_violin(draw_quantiles = c(0.25,0.5,0.75)) + xlab(&quot;Variable and bias evaluation interval&quot;) + ylab(&quot;Prediction error&quot;) It is also possible to evaluate the mean daily error: error_eval &lt;- data.frame(Variable = c(rep(&quot;Tmin&quot;,3),rep(&quot;Tmax&quot;,3)), Interval = rep(c(&quot;Year&quot;,&quot;Month&quot;,&quot;Two weeks&quot;),2), Error = c( mean(abs(Bonn_gaps$Tmin_annual[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE), mean(abs(Bonn_gaps$Tmin_month[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE), mean(abs(Bonn_gaps$Tmin_2weeks[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE), mean(abs(Bonn_gaps$Tmax_annual[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE), mean(abs(Bonn_gaps$Tmax_month[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE), mean(abs(Bonn_gaps$Tmax_2weeks[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE)) ) error_eval Variable Interval Error Tmin Year 0.9696349 Tmin Month 0.9615336 Tmin Two weeks 0.9652847 Tmax Year 0.8247899 Tmax Month 0.8059148 Tmax Two weeks 0.8048853 The improvement in the results was not very significant, likely because the stations used are relatively close to each other, and the weather conditions between them do not vary much. 11.4.2 Saving the data for later The dataset based on monthly intervals should be saved for later use. Before saving, the remaining missing day needs to be interpolated. Once this gap is filled, the dataset can be stored for future use. monthly_bias_fixed &lt;- fix_weather(patched_monthly) The long-term record for Bonn, covering the period from 1990 to 2020, is now complete with no remaining gaps. This dataset can now be saved for future use. write.csv(monthly_bias_fixed$weather, &quot;data/Bonn_weather.csv&quot;) Only the weather element, a data.frame, was saved, as it can be easily exported as a spreadsheet (.csv file). While it is also possible to save lists, such as the QC element, this requires a more complex process, so it will be saved at a later time. 11.5 Exercises on filling gaps Use chillR functions to find out how many gaps you have in your dataset (even if you have none, please still follow all further steps) Yakima &lt;- read.csv(&quot;Yakima/chillR_weather.csv&quot;) Yakima_QC &lt;- fix_weather(Yakima)$QC Season End_year Season_days Data_days Missing_Tmin Missing_Tmax Incomplete_days Perc_complete 1989/1990 1990 365 365 0 0 0 100 1990/1991 1991 365 365 0 0 0 100 1991/1992 1992 366 366 0 0 0 100 1992/1993 1993 365 365 0 0 0 100 1993/1994 1994 365 365 0 0 0 100 1994/1995 1995 365 365 0 0 0 100 1995/1996 1996 366 366 0 0 0 100 1996/1997 1997 365 365 0 0 0 100 1997/1998 1998 365 365 0 0 0 100 1998/1999 1999 365 365 0 0 0 100 1999/2000 2000 366 366 0 0 0 100 2000/2001 2001 365 365 0 0 0 100 2001/2002 2002 365 365 0 0 0 100 2002/2003 2003 365 365 0 0 0 100 2003/2004 2004 366 366 0 0 0 100 2004/2005 2005 365 365 0 0 0 100 2005/2006 2006 365 365 0 0 0 100 2006/2007 2007 365 365 0 0 0 100 2007/2008 2008 366 366 0 0 0 100 2008/2009 2009 365 365 0 0 0 100 2009/2010 2010 365 365 0 0 0 100 2010/2011 2011 365 365 0 0 0 100 2011/2012 2012 366 366 0 0 0 100 2012/2013 2013 365 365 0 0 0 100 2013/2014 2014 365 365 0 0 0 100 2014/2015 2015 365 365 0 0 0 100 2015/2016 2016 366 366 0 0 0 100 2016/2017 2017 365 365 0 0 0 100 2017/2018 2018 365 365 0 0 0 100 2018/2019 2019 365 365 0 0 0 100 2019/2020 2020 366 366 0 0 0 100 Create a list of the 25 closest weather stations using the handle_gsod function station_list_Yakima &lt;- handle_gsod(action = &quot;list_stations&quot;, location = c(long = -120.50, lat = 46.60), time_interval = c(1990, 2020)) chillR_code STATION.NAME CTRY Lat Long BEGIN END Distance Overlap_years Perc_interval_covered 72781024243 YAKIMA AIR TERMINAL/MCALSR FIELD AP US 46.564 -120.535 19730101 20250303 4.82 31.00 100 99999924243 YAKIMA AIR TERMINAL US 46.568 -120.543 19480101 19721231 4.85 0.00 0 72781399999 VAGABOND AAF / YAKIMA TRAINING CENTER WASHINGTON USA US 46.667 -120.454 20030617 20081110 8.25 5.40 17 72056299999 RANGE OP 13 / YAKIMA TRAINING CENTER US 46.800 -120.167 20080530 20170920 33.79 9.31 30 72788399999 BOWERS FLD US 47.033 -120.531 20000101 20031231 48.26 4.00 13 72788324220 BOWERS FIELD AIRPORT US 47.034 -120.531 19880106 20250303 48.37 31.00 100 99999924220 ELLENSBURG BOWERS FI US 47.034 -120.530 19480601 19550101 48.37 0.00 0 72784094187 HANFORD AIRPORT US 46.567 -119.600 20060101 20130326 68.96 7.23 23 72784099999 HANFORD US 46.567 -119.600 19730101 19971231 68.96 8.00 26 72782594239 PANGBORN MEMORIAL AIRPORT US 47.397 -120.201 20000101 20250303 91.58 21.00 68 72782599999 PANGBORN MEM US 47.399 -120.207 19730101 19971231 91.69 8.00 26 72788499999 RICHLAND AIRPORT US 46.306 -119.304 19810203 20250302 97.39 31.00 100 72781524237 STAMPASS PASS FLTWO US 47.277 -121.337 19730101 20250303 98.63 31.00 100 99999924237 STAMPEDE PASS US 47.277 -121.337 19480101 19721231 98.63 0.00 0 72790024141 EPHRATA MUNICIPAL AIRPORT US 47.308 -119.516 20050101 20250303 108.64 16.00 52 72782624141 EPHRATA MUNICIPAL US 47.308 -119.515 19420101 19971231 108.69 8.00 26 99999924141 EPHRATA AP FCWOS US 47.308 -119.515 19480101 19550101 108.69 0.00 0 72782724110 GRANT COUNTY INTL AIRPORT US 47.193 -119.315 19430610 20250303 111.73 31.00 100 72782799999 MOSES LAKE/GRANT CO US 47.200 -119.317 20000101 20031231 112.06 4.00 13 72784524163 TRI-CITIES AIRPORT US 46.270 -119.118 19730101 20250303 112.21 31.00 100 72784599999 TRI CITIES US 46.267 -119.117 20000101 20031231 112.40 4.00 13 99999924163 PASCO NAS US 46.267 -119.117 19450401 19460601 112.40 0.00 0 72698824219 MUNICIPAL AIRPORT US 45.619 -121.166 19730101 20250303 120.70 31.00 100 99999924219 THE DALLES MUNICIPAL ARPT US 45.619 -121.166 19480101 19650101 120.70 0.00 0 72688399999 HERMISTON MUNI US 45.828 -119.259 19980514 20051231 128.55 7.64 25 Identify suitable weather stations for patching gaps Download weather data for promising stations, convert them to chillR format and compile them in a list patch_weather &lt;- handle_gsod(action = &quot;download_weather&quot;, location = as.character(station_list_Yakima$chillR_code[c(4, 6, 8)]), time_interval = c(1990, 2020)) %&gt;% handle_gsod() ## Loading data for 31 years from station &#39;RANGE OP 13 / YAKIMA TRAINING CENTER&#39; ## ============================================================================================== ## ## Loading data for 31 years from station &#39;HANFORD AIRPORT&#39; ## ============================================================================================== ## ## Loading data for 31 years from station &#39;BOWERS FIELD AIRPORT&#39; ## ============================================================================================== Use the patch_daily_temperatures function to fill gaps patched &lt;- patch_daily_temperatures(weather = Yakima, patch_weather = patch_weather) # Patch statistics for YRANGE OP 13 /AKIMA TRAINING CENTER patched$statistics[[1]] mean_bias stdev_bias filled gaps_remain Tmin NA NA NA NA Tmax NA NA NA NA # Patch statistics for HANFORD AIRPORT patched$statistics[[2]] mean_bias stdev_bias filled gaps_remain Tmin NA NA NA NA Tmax NA NA NA NA # Patch statistics for BOWERS FIELD AIRPORT patched$statistics[[3]] mean_bias stdev_bias filled gaps_remain Tmin NA NA NA NA Tmax NA NA NA NA Investigate the results - have all gaps been filled? write.csv(patched$weather, &quot;Yakima/Yakima_weather.csv&quot;, row.names = FALSE) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
